{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Objective - This code is built for prediction of the returning shipment\n",
    "\n",
    "#  Description - The code will have basic 4 sections :\n",
    "##                         1. Data Import and basic Quality Checks\n",
    "##                         2. Exploratoy data analysis, adding features to data and making model ready data\n",
    "##                         3. Building Random Forrest and Neural Network Models   \n",
    "##                         4. Prediction of return rate using the validation data\n",
    "\n",
    "\n",
    "# Date -            07 July 2021\n",
    "# Author -        Avinash Mishra \n",
    "# Company -    XYZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import ta\n",
    "import warnings\n",
    "import time \n",
    "from IPython.display import clear_output\n",
    "#from pyti.exponential_moving_average import exponential_moving_average as EMA\n",
    "import xlrd \n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "import sys\n",
    "import decimal\n",
    "import pyodbc \n",
    "import pandas.io.sql\n",
    "import smtplib\n",
    "import mimetypes\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "#import paho.mqtt.client as mqtt\n",
    "import sys  \n",
    "import csv\n",
    "import socket\n",
    "from datetime import datetime\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.signal import find_peaks, peak_prominences\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import style\n",
    "from functools import reduce \n",
    "import re\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from scipy.spatial.distance import euclidean\n",
    "import glob\n",
    "from datetime import date, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import clear_output\n",
    "from numpy import arange\n",
    "import os \n",
    "from sklearn.metrics import r2_score\n",
    "import ta\n",
    "from ta import *\n",
    "import talib\n",
    "#from rfpimp import permutation_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section  - 1 - Basic Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_size</th>\n",
       "      <th>item_color</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_title</th>\n",
       "      <th>user_dob</th>\n",
       "      <th>user_state</th>\n",
       "      <th>user_reg_date</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>22-06-2016</td>\n",
       "      <td>27-06-2016</td>\n",
       "      <td>643</td>\n",
       "      <td>38</td>\n",
       "      <td>navy</td>\n",
       "      <td>30</td>\n",
       "      <td>49.9</td>\n",
       "      <td>30822</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>17-04-1969</td>\n",
       "      <td>1013</td>\n",
       "      <td>23-06-2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>22-06-2016</td>\n",
       "      <td>27-06-2016</td>\n",
       "      <td>195</td>\n",
       "      <td>xxl</td>\n",
       "      <td>grey</td>\n",
       "      <td>46</td>\n",
       "      <td>19.9</td>\n",
       "      <td>30823</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>22-04-1970</td>\n",
       "      <td>1001</td>\n",
       "      <td>15-03-2015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>22-06-2016</td>\n",
       "      <td>05-07-2016</td>\n",
       "      <td>25</td>\n",
       "      <td>xxl</td>\n",
       "      <td>grey</td>\n",
       "      <td>5</td>\n",
       "      <td>79.9</td>\n",
       "      <td>30823</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>22-04-1970</td>\n",
       "      <td>1001</td>\n",
       "      <td>15-03-2015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>23-06-2016</td>\n",
       "      <td>26-06-2016</td>\n",
       "      <td>173</td>\n",
       "      <td>m</td>\n",
       "      <td>brown</td>\n",
       "      <td>20</td>\n",
       "      <td>19.9</td>\n",
       "      <td>17234</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>09-01-1960</td>\n",
       "      <td>1013</td>\n",
       "      <td>17-02-2015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>23-06-2016</td>\n",
       "      <td>26-06-2016</td>\n",
       "      <td>394</td>\n",
       "      <td>40</td>\n",
       "      <td>black</td>\n",
       "      <td>44</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30827</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1006</td>\n",
       "      <td>09-02-2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_item_id  order_date delivery_date  item_id item_size item_color  \\\n",
       "0              1  22-06-2016    27-06-2016      643        38       navy   \n",
       "1             10  22-06-2016    27-06-2016      195       xxl       grey   \n",
       "2             11  22-06-2016    05-07-2016       25       xxl       grey   \n",
       "3             32  23-06-2016    26-06-2016      173         m      brown   \n",
       "4             43  23-06-2016    26-06-2016      394        40      black   \n",
       "\n",
       "   brand_id  item_price  user_id user_title    user_dob  user_state  \\\n",
       "0        30        49.9    30822        Mrs  17-04-1969        1013   \n",
       "1        46        19.9    30823        Mrs  22-04-1970        1001   \n",
       "2         5        79.9    30823        Mrs  22-04-1970        1001   \n",
       "3        20        19.9    17234        Mrs  09-01-1960        1013   \n",
       "4        44        90.0    30827        Mrs         NaN        1006   \n",
       "\n",
       "  user_reg_date  return  \n",
       "0    23-06-2016       0  \n",
       "1    15-03-2015       1  \n",
       "2    15-03-2015       0  \n",
       "3    17-02-2015       0  \n",
       "4    09-02-2016       1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:/Users/Avinash Mishra/Avinash/Python Codes/Teleperformance/\"\n",
    "df= pd.read_csv(path + \"TrainingData_V1.csv\")\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create some derived varibale in the data \n",
    "df['delivery_date']=pd.to_datetime(df['delivery_date'],format = '%d-%m-%Y')\n",
    "df['order_date']=pd.to_datetime(df['order_date'],format = '%d-%m-%Y')\n",
    "df['user_reg_date']=pd.to_datetime(df['user_reg_date'],format = '%d-%m-%Y')\n",
    "df['user_dob']=pd.to_datetime(df['user_dob'],format = '%d-%m-%Y')\n",
    "df['Today'] =pd.to_datetime(date.today(),format = '%Y-%m-%d')\n",
    "\n",
    "\n",
    "df['Delivery_Time'] = (df['delivery_date'] -df['order_date'])// timedelta(days=1)\n",
    "df['User_Age_years'] = (df['Today'] - df['user_dob'])// timedelta(days=365.2425)\n",
    "df['System_Age_months'] =(df['Today'] - df['user_reg_date'])// timedelta(days=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    72956.000000\n",
       "mean        56.729947\n",
       "std         11.259442\n",
       "min          9.000000\n",
       "25%         51.000000\n",
       "50%         56.000000\n",
       "75%         62.000000\n",
       "max        120.000000\n",
       "Name: User_Age_years, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['User_Age_years'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    79945.000000\n",
       "mean         6.306488\n",
       "std         15.813433\n",
       "min        -84.607387\n",
       "25%          2.000000\n",
       "50%          3.000000\n",
       "75%          6.306488\n",
       "max        173.000000\n",
       "Name: Delivery_Time, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Delivery_Time'] = np.where(df['Delivery_Time']< 0, df['Delivery_Time'].mean(),df['Delivery_Time'] )\n",
    "df['Delivery_Time'] = df['Delivery_Time'].fillna(df['Delivery_Time'].mean())\n",
    "df['Delivery_Time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delivery_date     9.301395\n",
       "user_dob          8.742260\n",
       "User_Age_years    8.742260\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the columns with na values \n",
    "df[df.columns[df.isnull().any()]].isnull().sum() * 100 / df.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section  2 : Exploratory Analysis to understand the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_size</th>\n",
       "      <th>item_color</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_title</th>\n",
       "      <th>user_dob</th>\n",
       "      <th>user_state</th>\n",
       "      <th>user_reg_date</th>\n",
       "      <th>return</th>\n",
       "      <th>Today</th>\n",
       "      <th>Delivery_Time</th>\n",
       "      <th>User_Age_years</th>\n",
       "      <th>System_Age_months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>643</td>\n",
       "      <td>38</td>\n",
       "      <td>navy</td>\n",
       "      <td>30</td>\n",
       "      <td>49.90</td>\n",
       "      <td>30822</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1969-04-17</td>\n",
       "      <td>1013</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>52.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60177</td>\n",
       "      <td>650</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>106</td>\n",
       "      <td>38</td>\n",
       "      <td>blue</td>\n",
       "      <td>6</td>\n",
       "      <td>39.90</td>\n",
       "      <td>30931</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1970-03-29</td>\n",
       "      <td>1010</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9976</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>337</td>\n",
       "      <td>152</td>\n",
       "      <td>grey</td>\n",
       "      <td>30</td>\n",
       "      <td>19.95</td>\n",
       "      <td>30822</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1969-04-17</td>\n",
       "      <td>1013</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>6.306488</td>\n",
       "      <td>52.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40080</td>\n",
       "      <td>574</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>2016-07-18</td>\n",
       "      <td>315</td>\n",
       "      <td>39</td>\n",
       "      <td>grey</td>\n",
       "      <td>45</td>\n",
       "      <td>89.90</td>\n",
       "      <td>9214</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1959-02-23</td>\n",
       "      <td>1015</td>\n",
       "      <td>2015-05-13</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40079</td>\n",
       "      <td>563</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>423</td>\n",
       "      <td>xl</td>\n",
       "      <td>red</td>\n",
       "      <td>33</td>\n",
       "      <td>12.90</td>\n",
       "      <td>30920</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1952-05-24</td>\n",
       "      <td>1002</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>69.0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79940</th>\n",
       "      <td>50076</td>\n",
       "      <td>99823</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-13</td>\n",
       "      <td>2048</td>\n",
       "      <td>xxl</td>\n",
       "      <td>olive</td>\n",
       "      <td>75</td>\n",
       "      <td>249.90</td>\n",
       "      <td>48213</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1944-08-10</td>\n",
       "      <td>1015</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>76.0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79941</th>\n",
       "      <td>50075</td>\n",
       "      <td>99821</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-13</td>\n",
       "      <td>2127</td>\n",
       "      <td>unsized</td>\n",
       "      <td>black</td>\n",
       "      <td>48</td>\n",
       "      <td>19.95</td>\n",
       "      <td>48213</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1944-08-10</td>\n",
       "      <td>1015</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>76.0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79942</th>\n",
       "      <td>50074</td>\n",
       "      <td>99806</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>292</td>\n",
       "      <td>6+</td>\n",
       "      <td>grey</td>\n",
       "      <td>4</td>\n",
       "      <td>79.90</td>\n",
       "      <td>48209</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1970-01-31</td>\n",
       "      <td>1013</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79943</th>\n",
       "      <td>70072</td>\n",
       "      <td>99999</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>1994-12-31</td>\n",
       "      <td>1832</td>\n",
       "      <td>xxl</td>\n",
       "      <td>black</td>\n",
       "      <td>37</td>\n",
       "      <td>26.90</td>\n",
       "      <td>47794</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1985-03-01</td>\n",
       "      <td>1006</td>\n",
       "      <td>2016-09-10</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>-84.607387</td>\n",
       "      <td>36.0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79944</th>\n",
       "      <td>79944</td>\n",
       "      <td>99997</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>156</td>\n",
       "      <td>20</td>\n",
       "      <td>brown</td>\n",
       "      <td>34</td>\n",
       "      <td>29.90</td>\n",
       "      <td>713</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1959-03-21</td>\n",
       "      <td>1011</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79945 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  order_item_id order_date delivery_date  item_id item_size  \\\n",
       "0          0              1 2016-06-22    2016-06-27      643        38   \n",
       "1      60177            650 2016-06-22    2016-06-28      106        38   \n",
       "2       9976              2 2016-06-22           NaT      337       152   \n",
       "3      40080            574 2016-06-22    2016-07-18      315        39   \n",
       "4      40079            563 2016-06-22    2016-06-26      423        xl   \n",
       "...      ...            ...        ...           ...      ...       ...   \n",
       "79940  50076          99823 2016-09-11    2016-09-13     2048       xxl   \n",
       "79941  50075          99821 2016-09-11    2016-09-13     2127   unsized   \n",
       "79942  50074          99806 2016-09-11    2016-09-12      292        6+   \n",
       "79943  70072          99999 2016-09-11    1994-12-31     1832       xxl   \n",
       "79944  79944          99997 2016-09-11    2016-09-12      156        20   \n",
       "\n",
       "      item_color  brand_id  item_price  user_id user_title   user_dob  \\\n",
       "0           navy        30       49.90    30822        Mrs 1969-04-17   \n",
       "1           blue         6       39.90    30931         Mr 1970-03-29   \n",
       "2           grey        30       19.95    30822        Mrs 1969-04-17   \n",
       "3           grey        45       89.90     9214        Mrs 1959-02-23   \n",
       "4            red        33       12.90    30920        Mrs 1952-05-24   \n",
       "...          ...       ...         ...      ...        ...        ...   \n",
       "79940      olive        75      249.90    48213         Mr 1944-08-10   \n",
       "79941      black        48       19.95    48213         Mr 1944-08-10   \n",
       "79942       grey         4       79.90    48209        Mrs 1970-01-31   \n",
       "79943      black        37       26.90    47794         Mr 1985-03-01   \n",
       "79944      brown        34       29.90      713        Mrs 1959-03-21   \n",
       "\n",
       "       user_state user_reg_date  return      Today  Delivery_Time  \\\n",
       "0            1013    2016-06-23       0 2021-07-06       5.000000   \n",
       "1            1010    2015-02-17       0 2021-07-06       6.000000   \n",
       "2            1013    2016-06-23       0 2021-07-06       6.306488   \n",
       "3            1015    2015-05-13       1 2021-07-06      26.000000   \n",
       "4            1002    2015-02-17       1 2021-07-06       4.000000   \n",
       "...           ...           ...     ...        ...            ...   \n",
       "79940        1015    2015-02-17       1 2021-07-06       2.000000   \n",
       "79941        1015    2015-02-17       1 2021-07-06       2.000000   \n",
       "79942        1013    2016-09-12       1 2021-07-06       1.000000   \n",
       "79943        1006    2016-09-10       0 2021-07-06     -84.607387   \n",
       "79944        1011    2015-02-17       0 2021-07-06       1.000000   \n",
       "\n",
       "       User_Age_years  System_Age_months  \n",
       "0                52.0                 61  \n",
       "1                51.0                 77  \n",
       "2                52.0                 61  \n",
       "3                62.0                 74  \n",
       "4                69.0                 77  \n",
       "...               ...                ...  \n",
       "79940            76.0                 77  \n",
       "79941            76.0                 77  \n",
       "79942            51.0                 58  \n",
       "79943            36.0                 58  \n",
       "79944            62.0                 77  \n",
       "\n",
       "[79945 rows x 19 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =df.sort_values(by=['order_date'], ascending=True)\n",
    "df= df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adding some derived variables in the data\n",
    "\n",
    "#### Creating a variable for the user return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#df['User_level_return_count'] = df.groupby(by=['user_id','order_date'])['return'].sum()\n",
    "\n",
    "#### Creating historical return history at a user level \n",
    "\n",
    "test = pd.DataFrame(df.groupby(by=['user_id','order_date'])['return'].sum())\n",
    "test =test.reset_index()\n",
    "test.columns = ['user_id','order_date','return']\n",
    "test['user_return_history'] = test.groupby(['user_id'])['return'].transform(pd.Series.cumsum)\n",
    "test['user_return_history'] =test.groupby('user_id')['user_return_history'].shift().fillna(0)\n",
    "test=test[['order_date','user_id','user_return_history']]\n",
    "\n",
    "df = df.merge(test, how='inner', on=['order_date','user_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent historical return rates by item,brand,size,color and state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Recent historical return at an item and brand level \n",
    "\n",
    "#### list of variable to create return trend \n",
    "\n",
    "var = ['brand_id','item_id','item_size','user_state','item_color']\n",
    "\n",
    "for each in var:\n",
    "\n",
    "\n",
    "    test = pd.DataFrame()\n",
    "    test =df.groupby(by=[each,'order_date']).agg(Total_Returns=('return', 'sum'),Total_Orders=('return', 'count'))\n",
    "    test['Return_per'] = test['Total_Returns']/test['Total_Orders']\n",
    "    test[str(each) + \"_return_trend\"] = test['Return_per'].rolling(3).mean()\n",
    "    test[str(each) + \"_return_trend\"] =test.groupby(by =[each])[str(each) + \"_return_trend\"].shift().fillna(0)\n",
    "\n",
    "    test = test.reset_index()\n",
    "    test = test[['order_date',each,str(each) + \"_return_trend\"]]\n",
    "\n",
    "    df = df.merge(test, how='inner', on=['order_date',each])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_size</th>\n",
       "      <th>item_color</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>Today</th>\n",
       "      <th>Delivery_Time</th>\n",
       "      <th>User_Age_years</th>\n",
       "      <th>System_Age_months</th>\n",
       "      <th>user_return_history</th>\n",
       "      <th>brand_id_return_trend</th>\n",
       "      <th>item_id_return_trend</th>\n",
       "      <th>item_size_return_trend</th>\n",
       "      <th>user_state_return_trend</th>\n",
       "      <th>item_color_return_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>643</td>\n",
       "      <td>38</td>\n",
       "      <td>navy</td>\n",
       "      <td>30</td>\n",
       "      <td>49.90</td>\n",
       "      <td>30822</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>52.0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9976</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>337</td>\n",
       "      <td>152</td>\n",
       "      <td>grey</td>\n",
       "      <td>30</td>\n",
       "      <td>19.95</td>\n",
       "      <td>30822</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>6.306488</td>\n",
       "      <td>52.0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19997</td>\n",
       "      <td>605</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>425</td>\n",
       "      <td>xxl</td>\n",
       "      <td>grey</td>\n",
       "      <td>46</td>\n",
       "      <td>24.90</td>\n",
       "      <td>30928</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30063</td>\n",
       "      <td>604</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>358</td>\n",
       "      <td>xxl</td>\n",
       "      <td>grey</td>\n",
       "      <td>74</td>\n",
       "      <td>39.90</td>\n",
       "      <td>30928</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>25</td>\n",
       "      <td>xxl</td>\n",
       "      <td>grey</td>\n",
       "      <td>5</td>\n",
       "      <td>79.90</td>\n",
       "      <td>30823</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79940</th>\n",
       "      <td>19943</td>\n",
       "      <td>99971</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-13</td>\n",
       "      <td>324</td>\n",
       "      <td>40</td>\n",
       "      <td>ocher</td>\n",
       "      <td>44</td>\n",
       "      <td>85.00</td>\n",
       "      <td>48237</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.527277</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.409524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79941</th>\n",
       "      <td>60106</td>\n",
       "      <td>99928</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>1994-12-31</td>\n",
       "      <td>71</td>\n",
       "      <td>9+</td>\n",
       "      <td>ocher</td>\n",
       "      <td>21</td>\n",
       "      <td>49.95</td>\n",
       "      <td>45237</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>-84.607387</td>\n",
       "      <td>35.0</td>\n",
       "      <td>59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.672222</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.510303</td>\n",
       "      <td>0.409524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79942</th>\n",
       "      <td>60110</td>\n",
       "      <td>99967</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-13</td>\n",
       "      <td>363</td>\n",
       "      <td>40</td>\n",
       "      <td>ecru</td>\n",
       "      <td>40</td>\n",
       "      <td>64.90</td>\n",
       "      <td>48237</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.668651</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.527277</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79943</th>\n",
       "      <td>40007</td>\n",
       "      <td>99968</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-13</td>\n",
       "      <td>259</td>\n",
       "      <td>40</td>\n",
       "      <td>mint</td>\n",
       "      <td>1</td>\n",
       "      <td>89.90</td>\n",
       "      <td>48237</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541025</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.527277</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79944</th>\n",
       "      <td>29997</td>\n",
       "      <td>99972</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-13</td>\n",
       "      <td>324</td>\n",
       "      <td>39</td>\n",
       "      <td>turquoise</td>\n",
       "      <td>44</td>\n",
       "      <td>85.00</td>\n",
       "      <td>48237</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521088</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79945 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  order_item_id order_date delivery_date  item_id item_size  \\\n",
       "0          0              1 2016-06-22    2016-06-27      643        38   \n",
       "1       9976              2 2016-06-22           NaT      337       152   \n",
       "2      19997            605 2016-06-22    2016-06-27      425       xxl   \n",
       "3      30063            604 2016-06-22    2016-06-27      358       xxl   \n",
       "4          2             11 2016-06-22    2016-07-05       25       xxl   \n",
       "...      ...            ...        ...           ...      ...       ...   \n",
       "79940  19943          99971 2016-09-11    2016-09-13      324        40   \n",
       "79941  60106          99928 2016-09-11    1994-12-31       71        9+   \n",
       "79942  60110          99967 2016-09-11    2016-09-13      363        40   \n",
       "79943  40007          99968 2016-09-11    2016-09-13      259        40   \n",
       "79944  29997          99972 2016-09-11    2016-09-13      324        39   \n",
       "\n",
       "      item_color  brand_id  item_price  user_id  ...      Today Delivery_Time  \\\n",
       "0           navy        30       49.90    30822  ... 2021-07-06      5.000000   \n",
       "1           grey        30       19.95    30822  ... 2021-07-06      6.306488   \n",
       "2           grey        46       24.90    30928  ... 2021-07-06      5.000000   \n",
       "3           grey        74       39.90    30928  ... 2021-07-06      5.000000   \n",
       "4           grey         5       79.90    30823  ... 2021-07-06     13.000000   \n",
       "...          ...       ...         ...      ...  ...        ...           ...   \n",
       "79940      ocher        44       85.00    48237  ... 2021-07-06      2.000000   \n",
       "79941      ocher        21       49.95    45237  ... 2021-07-06    -84.607387   \n",
       "79942       ecru        40       64.90    48237  ... 2021-07-06      2.000000   \n",
       "79943       mint         1       89.90    48237  ... 2021-07-06      2.000000   \n",
       "79944  turquoise        44       85.00    48237  ... 2021-07-06      2.000000   \n",
       "\n",
       "       User_Age_years System_Age_months  user_return_history  \\\n",
       "0                52.0                61                  0.0   \n",
       "1                52.0                61                  0.0   \n",
       "2                75.0                72                  0.0   \n",
       "3                75.0                72                  0.0   \n",
       "4                51.0                76                  0.0   \n",
       "...               ...               ...                  ...   \n",
       "79940            55.0                77                  0.0   \n",
       "79941            35.0                59                  1.0   \n",
       "79942            55.0                77                  0.0   \n",
       "79943            55.0                77                  0.0   \n",
       "79944            55.0                77                  0.0   \n",
       "\n",
       "      brand_id_return_trend  item_id_return_trend  item_size_return_trend  \\\n",
       "0                  0.000000              0.000000                0.000000   \n",
       "1                  0.000000              0.000000                0.000000   \n",
       "2                  0.000000              0.000000                0.000000   \n",
       "3                  0.000000              0.000000                0.000000   \n",
       "4                  0.000000              0.000000                0.000000   \n",
       "...                     ...                   ...                     ...   \n",
       "79940              1.000000              1.000000                0.527277   \n",
       "79941              0.672222              0.777778                0.666667   \n",
       "79942              0.668651              0.444444                0.527277   \n",
       "79943              0.541025              0.666667                0.527277   \n",
       "79944              1.000000              1.000000                0.521088   \n",
       "\n",
       "       user_state_return_trend  item_color_return_trend  \n",
       "0                     0.000000                 0.000000  \n",
       "1                     0.000000                 0.000000  \n",
       "2                     0.000000                 0.000000  \n",
       "3                     0.000000                 0.000000  \n",
       "4                     0.000000                 0.000000  \n",
       "...                        ...                      ...  \n",
       "79940                 0.355311                 0.409524  \n",
       "79941                 0.510303                 0.409524  \n",
       "79942                 0.355311                 1.000000  \n",
       "79943                 0.355311                 0.333333  \n",
       "79944                 0.355311                 0.500000  \n",
       "\n",
       "[79945 rows x 25 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting return rates by various variables to analysed the differentiating variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['order_date', 'item_id', 'item_size',\n",
    "       'item_color', 'brand_id', 'item_price', 'user_id', 'user_title',\n",
    "       'user_state', 'Delivery_Time',\n",
    "        'System_Age_months', 'User_Age_years','user_return_history','brand_id_return_trend','item_id_return_trend','item_size_return_trend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_date\n",
      "item_id\n",
      "item_size\n",
      "item_color\n",
      "brand_id\n",
      "item_price\n",
      "user_id\n",
      "user_title\n",
      "user_state\n",
      "Delivery_Time\n",
      "System_Age_months\n",
      "User_Age_years\n",
      "user_return_history\n",
      "brand_id_return_trend\n",
      "item_id_return_trend\n",
      "item_size_return_trend\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with pd.ExcelWriter(path + 'Return_rates_groups_20210704v1.xlsx') as writer:\n",
    "    \n",
    "    for each in col:\n",
    "\n",
    "        print(each)\n",
    "\n",
    "        test = pd.DataFrame\n",
    "\n",
    "        test = df.groupby([each]).agg(Total_Returns=('return', 'sum'),Total_Orders=('return', 'count'))\n",
    "        test['Return_per'] = test['Total_Returns']/test['Total_Orders']\n",
    "        test.to_excel(writer, sheet_name= each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bins = [0, 20, 40,60,80, 100]\n",
    "#df['binned_age'] = np.searchsorted(bins, df['User_Age_years'].values)\n",
    "#df['binned_System_Age'] = np.searchsorted(bins, df['System_Age_months'].values)\n",
    "#df['binned_Delivery_Time'] = np.searchsorted(bins, df['Delivery_Time'].values)\n",
    "#df['binned_item_price'] = np.searchsorted(bins, df['item_price'].values)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 - Building the models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dummy variables for the categorical variables in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Get the return type variable in the \n",
    "\n",
    "df_updated =pd.get_dummies(df, columns=[\"item_id\",\"item_size\",\"item_color\",\"brand_id\",\"user_title\",\"user_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated ['Price'] = df_updated ['item_price'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delivery_Time</th>\n",
       "      <th>User_Age_years</th>\n",
       "      <th>System_Age_months</th>\n",
       "      <th>user_return_history</th>\n",
       "      <th>brand_id_return_trend</th>\n",
       "      <th>item_id_return_trend</th>\n",
       "      <th>item_size_return_trend</th>\n",
       "      <th>user_state_return_trend</th>\n",
       "      <th>item_color_return_trend</th>\n",
       "      <th>item_id_1</th>\n",
       "      <th>...</th>\n",
       "      <th>user_state_1008</th>\n",
       "      <th>user_state_1009</th>\n",
       "      <th>user_state_1010</th>\n",
       "      <th>user_state_1011</th>\n",
       "      <th>user_state_1012</th>\n",
       "      <th>user_state_1013</th>\n",
       "      <th>user_state_1014</th>\n",
       "      <th>user_state_1015</th>\n",
       "      <th>user_state_1016</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>52.0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66157</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.527277</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66158</th>\n",
       "      <td>-84.607387</td>\n",
       "      <td>35.0</td>\n",
       "      <td>59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.672222</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.510303</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66159</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.668651</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.527277</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66160</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541025</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.527277</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66161</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521088</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66162 rows × 2246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Delivery_Time  User_Age_years  System_Age_months  user_return_history  \\\n",
       "0           5.000000            52.0                 61                  0.0   \n",
       "1           5.000000            75.0                 72                  0.0   \n",
       "2           5.000000            75.0                 72                  0.0   \n",
       "3          13.000000            51.0                 76                  0.0   \n",
       "4           5.000000            51.0                 76                  0.0   \n",
       "...              ...             ...                ...                  ...   \n",
       "66157       2.000000            55.0                 77                  0.0   \n",
       "66158     -84.607387            35.0                 59                  1.0   \n",
       "66159       2.000000            55.0                 77                  0.0   \n",
       "66160       2.000000            55.0                 77                  0.0   \n",
       "66161       2.000000            55.0                 77                  0.0   \n",
       "\n",
       "       brand_id_return_trend  item_id_return_trend  item_size_return_trend  \\\n",
       "0                   0.000000              0.000000                0.000000   \n",
       "1                   0.000000              0.000000                0.000000   \n",
       "2                   0.000000              0.000000                0.000000   \n",
       "3                   0.000000              0.000000                0.000000   \n",
       "4                   0.000000              0.000000                0.000000   \n",
       "...                      ...                   ...                     ...   \n",
       "66157               1.000000              1.000000                0.527277   \n",
       "66158               0.672222              0.777778                0.666667   \n",
       "66159               0.668651              0.444444                0.527277   \n",
       "66160               0.541025              0.666667                0.527277   \n",
       "66161               1.000000              1.000000                0.521088   \n",
       "\n",
       "       user_state_return_trend  item_color_return_trend  item_id_1  ...  \\\n",
       "0                     0.000000                 0.000000          0  ...   \n",
       "1                     0.000000                 0.000000          0  ...   \n",
       "2                     0.000000                 0.000000          0  ...   \n",
       "3                     0.000000                 0.000000          0  ...   \n",
       "4                     0.000000                 0.000000          0  ...   \n",
       "...                        ...                      ...        ...  ...   \n",
       "66157                 0.355311                 0.409524          0  ...   \n",
       "66158                 0.510303                 0.409524          0  ...   \n",
       "66159                 0.355311                 1.000000          0  ...   \n",
       "66160                 0.355311                 0.333333          0  ...   \n",
       "66161                 0.355311                 0.500000          0  ...   \n",
       "\n",
       "       user_state_1008  user_state_1009  user_state_1010  user_state_1011  \\\n",
       "0                    0                0                0                0   \n",
       "1                    0                0                1                0   \n",
       "2                    0                0                1                0   \n",
       "3                    0                0                0                0   \n",
       "4                    0                0                0                0   \n",
       "...                ...              ...              ...              ...   \n",
       "66157                0                0                0                0   \n",
       "66158                0                0                0                0   \n",
       "66159                0                0                0                0   \n",
       "66160                0                0                0                0   \n",
       "66161                0                0                0                0   \n",
       "\n",
       "       user_state_1012  user_state_1013  user_state_1014  user_state_1015  \\\n",
       "0                    0                1                0                0   \n",
       "1                    0                0                0                0   \n",
       "2                    0                0                0                0   \n",
       "3                    0                0                0                0   \n",
       "4                    0                0                0                0   \n",
       "...                ...              ...              ...              ...   \n",
       "66157                0                0                0                0   \n",
       "66158                0                0                0                0   \n",
       "66159                0                0                0                0   \n",
       "66160                0                0                0                0   \n",
       "66161                0                0                0                0   \n",
       "\n",
       "       user_state_1016  Price  \n",
       "0                    0  49.90  \n",
       "1                    0  24.90  \n",
       "2                    0  39.90  \n",
       "3                    0  79.90  \n",
       "4                    0  19.90  \n",
       "...                ...    ...  \n",
       "66157                0  85.00  \n",
       "66158                0  49.95  \n",
       "66159                0  64.90  \n",
       "66160                0  89.90  \n",
       "66161                0  85.00  \n",
       "\n",
       "[66162 rows x 2246 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_updated = df_updated.dropna()\n",
    "df_updated = df_updated.reset_index()\n",
    "df_updated.iloc[:,11:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the data for the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc = StandardScaler()\n",
    "X =  pd.DataFrame(sc.fit_transform(df_updated.iloc[:,11:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delivery_Time</th>\n",
       "      <th>User_Age_years</th>\n",
       "      <th>System_Age_months</th>\n",
       "      <th>user_return_history</th>\n",
       "      <th>brand_id_return_trend</th>\n",
       "      <th>item_id_return_trend</th>\n",
       "      <th>item_size_return_trend</th>\n",
       "      <th>user_state_return_trend</th>\n",
       "      <th>item_color_return_trend</th>\n",
       "      <th>item_id_1</th>\n",
       "      <th>...</th>\n",
       "      <th>user_state_1008</th>\n",
       "      <th>user_state_1009</th>\n",
       "      <th>user_state_1010</th>\n",
       "      <th>user_state_1011</th>\n",
       "      <th>user_state_1012</th>\n",
       "      <th>user_state_1013</th>\n",
       "      <th>user_state_1014</th>\n",
       "      <th>user_state_1015</th>\n",
       "      <th>user_state_1016</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.076701</td>\n",
       "      <td>-0.421599</td>\n",
       "      <td>-1.122896</td>\n",
       "      <td>-0.335554</td>\n",
       "      <td>-2.997945</td>\n",
       "      <td>-1.722669</td>\n",
       "      <td>-4.007651</td>\n",
       "      <td>-5.779201</td>\n",
       "      <td>-4.593846</td>\n",
       "      <td>-0.038907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420375</td>\n",
       "      <td>-0.128941</td>\n",
       "      <td>-0.544430</td>\n",
       "      <td>-0.226403</td>\n",
       "      <td>-0.095099</td>\n",
       "      <td>5.497565</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.241533</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.323767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.076701</td>\n",
       "      <td>1.618129</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>-0.335554</td>\n",
       "      <td>-2.997945</td>\n",
       "      <td>-1.722669</td>\n",
       "      <td>-4.007651</td>\n",
       "      <td>-5.779201</td>\n",
       "      <td>-4.593846</td>\n",
       "      <td>-0.038907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420375</td>\n",
       "      <td>-0.128941</td>\n",
       "      <td>1.836782</td>\n",
       "      <td>-0.226403</td>\n",
       "      <td>-0.095099</td>\n",
       "      <td>-0.181899</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.241533</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.845079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.076701</td>\n",
       "      <td>1.618129</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>-0.335554</td>\n",
       "      <td>-2.997945</td>\n",
       "      <td>-1.722669</td>\n",
       "      <td>-4.007651</td>\n",
       "      <td>-5.779201</td>\n",
       "      <td>-4.593846</td>\n",
       "      <td>-0.038907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420375</td>\n",
       "      <td>-0.128941</td>\n",
       "      <td>1.836782</td>\n",
       "      <td>-0.226403</td>\n",
       "      <td>-0.095099</td>\n",
       "      <td>-0.181899</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.241533</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.532292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.404797</td>\n",
       "      <td>-0.510283</td>\n",
       "      <td>0.928144</td>\n",
       "      <td>-0.335554</td>\n",
       "      <td>-2.997945</td>\n",
       "      <td>-1.722669</td>\n",
       "      <td>-4.007651</td>\n",
       "      <td>-5.779201</td>\n",
       "      <td>-4.593846</td>\n",
       "      <td>-0.038907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420375</td>\n",
       "      <td>-0.128941</td>\n",
       "      <td>-0.544430</td>\n",
       "      <td>-0.226403</td>\n",
       "      <td>-0.095099</td>\n",
       "      <td>-0.181899</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.241533</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>0.301809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.076701</td>\n",
       "      <td>-0.510283</td>\n",
       "      <td>0.928144</td>\n",
       "      <td>-0.335554</td>\n",
       "      <td>-2.997945</td>\n",
       "      <td>-1.722669</td>\n",
       "      <td>-4.007651</td>\n",
       "      <td>-5.779201</td>\n",
       "      <td>-4.593846</td>\n",
       "      <td>-0.038907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420375</td>\n",
       "      <td>-0.128941</td>\n",
       "      <td>-0.544430</td>\n",
       "      <td>-0.226403</td>\n",
       "      <td>-0.095099</td>\n",
       "      <td>-0.181899</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.241533</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.949342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66157</th>\n",
       "      <td>-0.257262</td>\n",
       "      <td>-0.155547</td>\n",
       "      <td>1.064880</td>\n",
       "      <td>-0.335554</td>\n",
       "      <td>3.503164</td>\n",
       "      <td>2.144643</td>\n",
       "      <td>0.592515</td>\n",
       "      <td>-1.289860</td>\n",
       "      <td>-0.511258</td>\n",
       "      <td>-0.038907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420375</td>\n",
       "      <td>-0.128941</td>\n",
       "      <td>-0.544430</td>\n",
       "      <td>-0.226403</td>\n",
       "      <td>-0.095099</td>\n",
       "      <td>-0.181899</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.241533</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>0.408156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66158</th>\n",
       "      <td>-5.469917</td>\n",
       "      <td>-1.929224</td>\n",
       "      <td>-1.396368</td>\n",
       "      <td>-0.007342</td>\n",
       "      <td>1.372245</td>\n",
       "      <td>1.285240</td>\n",
       "      <td>1.808604</td>\n",
       "      <td>0.668452</td>\n",
       "      <td>-0.511258</td>\n",
       "      <td>-0.038907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420375</td>\n",
       "      <td>-0.128941</td>\n",
       "      <td>-0.544430</td>\n",
       "      <td>-0.226403</td>\n",
       "      <td>-0.095099</td>\n",
       "      <td>-0.181899</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.241533</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.322724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66159</th>\n",
       "      <td>-0.257262</td>\n",
       "      <td>-0.155547</td>\n",
       "      <td>1.064880</td>\n",
       "      <td>-0.335554</td>\n",
       "      <td>1.349027</td>\n",
       "      <td>-0.003864</td>\n",
       "      <td>0.592515</td>\n",
       "      <td>-1.289860</td>\n",
       "      <td>5.375264</td>\n",
       "      <td>-0.038907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420375</td>\n",
       "      <td>-0.128941</td>\n",
       "      <td>-0.544430</td>\n",
       "      <td>-0.226403</td>\n",
       "      <td>-0.095099</td>\n",
       "      <td>-0.181899</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.241533</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>-0.010979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66160</th>\n",
       "      <td>-0.257262</td>\n",
       "      <td>-0.155547</td>\n",
       "      <td>1.064880</td>\n",
       "      <td>-0.335554</td>\n",
       "      <td>0.519319</td>\n",
       "      <td>0.855539</td>\n",
       "      <td>0.592515</td>\n",
       "      <td>-1.289860</td>\n",
       "      <td>-1.270809</td>\n",
       "      <td>-0.038907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420375</td>\n",
       "      <td>-0.128941</td>\n",
       "      <td>-0.544430</td>\n",
       "      <td>-0.226403</td>\n",
       "      <td>-0.095099</td>\n",
       "      <td>-0.181899</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.241533</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>0.510334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66161</th>\n",
       "      <td>-0.257262</td>\n",
       "      <td>-0.155547</td>\n",
       "      <td>1.064880</td>\n",
       "      <td>-0.335554</td>\n",
       "      <td>3.503164</td>\n",
       "      <td>2.144643</td>\n",
       "      <td>0.538524</td>\n",
       "      <td>-1.289860</td>\n",
       "      <td>0.390709</td>\n",
       "      <td>-0.038907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420375</td>\n",
       "      <td>-0.128941</td>\n",
       "      <td>-0.544430</td>\n",
       "      <td>-0.226403</td>\n",
       "      <td>-0.095099</td>\n",
       "      <td>-0.181899</td>\n",
       "      <td>-0.104448</td>\n",
       "      <td>-0.241533</td>\n",
       "      <td>-0.128212</td>\n",
       "      <td>0.408156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66162 rows × 2246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Delivery_Time  User_Age_years  System_Age_months  user_return_history  \\\n",
       "0          -0.076701       -0.421599          -1.122896            -0.335554   \n",
       "1          -0.076701        1.618129           0.381200            -0.335554   \n",
       "2          -0.076701        1.618129           0.381200            -0.335554   \n",
       "3           0.404797       -0.510283           0.928144            -0.335554   \n",
       "4          -0.076701       -0.510283           0.928144            -0.335554   \n",
       "...              ...             ...                ...                  ...   \n",
       "66157      -0.257262       -0.155547           1.064880            -0.335554   \n",
       "66158      -5.469917       -1.929224          -1.396368            -0.007342   \n",
       "66159      -0.257262       -0.155547           1.064880            -0.335554   \n",
       "66160      -0.257262       -0.155547           1.064880            -0.335554   \n",
       "66161      -0.257262       -0.155547           1.064880            -0.335554   \n",
       "\n",
       "       brand_id_return_trend  item_id_return_trend  item_size_return_trend  \\\n",
       "0                  -2.997945             -1.722669               -4.007651   \n",
       "1                  -2.997945             -1.722669               -4.007651   \n",
       "2                  -2.997945             -1.722669               -4.007651   \n",
       "3                  -2.997945             -1.722669               -4.007651   \n",
       "4                  -2.997945             -1.722669               -4.007651   \n",
       "...                      ...                   ...                     ...   \n",
       "66157               3.503164              2.144643                0.592515   \n",
       "66158               1.372245              1.285240                1.808604   \n",
       "66159               1.349027             -0.003864                0.592515   \n",
       "66160               0.519319              0.855539                0.592515   \n",
       "66161               3.503164              2.144643                0.538524   \n",
       "\n",
       "       user_state_return_trend  item_color_return_trend  item_id_1  ...  \\\n",
       "0                    -5.779201                -4.593846  -0.038907  ...   \n",
       "1                    -5.779201                -4.593846  -0.038907  ...   \n",
       "2                    -5.779201                -4.593846  -0.038907  ...   \n",
       "3                    -5.779201                -4.593846  -0.038907  ...   \n",
       "4                    -5.779201                -4.593846  -0.038907  ...   \n",
       "...                        ...                      ...        ...  ...   \n",
       "66157                -1.289860                -0.511258  -0.038907  ...   \n",
       "66158                 0.668452                -0.511258  -0.038907  ...   \n",
       "66159                -1.289860                 5.375264  -0.038907  ...   \n",
       "66160                -1.289860                -1.270809  -0.038907  ...   \n",
       "66161                -1.289860                 0.390709  -0.038907  ...   \n",
       "\n",
       "       user_state_1008  user_state_1009  user_state_1010  user_state_1011  \\\n",
       "0            -0.420375        -0.128941        -0.544430        -0.226403   \n",
       "1            -0.420375        -0.128941         1.836782        -0.226403   \n",
       "2            -0.420375        -0.128941         1.836782        -0.226403   \n",
       "3            -0.420375        -0.128941        -0.544430        -0.226403   \n",
       "4            -0.420375        -0.128941        -0.544430        -0.226403   \n",
       "...                ...              ...              ...              ...   \n",
       "66157        -0.420375        -0.128941        -0.544430        -0.226403   \n",
       "66158        -0.420375        -0.128941        -0.544430        -0.226403   \n",
       "66159        -0.420375        -0.128941        -0.544430        -0.226403   \n",
       "66160        -0.420375        -0.128941        -0.544430        -0.226403   \n",
       "66161        -0.420375        -0.128941        -0.544430        -0.226403   \n",
       "\n",
       "       user_state_1012  user_state_1013  user_state_1014  user_state_1015  \\\n",
       "0            -0.095099         5.497565        -0.104448        -0.241533   \n",
       "1            -0.095099        -0.181899        -0.104448        -0.241533   \n",
       "2            -0.095099        -0.181899        -0.104448        -0.241533   \n",
       "3            -0.095099        -0.181899        -0.104448        -0.241533   \n",
       "4            -0.095099        -0.181899        -0.104448        -0.241533   \n",
       "...                ...              ...              ...              ...   \n",
       "66157        -0.095099        -0.181899        -0.104448        -0.241533   \n",
       "66158        -0.095099        -0.181899        -0.104448        -0.241533   \n",
       "66159        -0.095099        -0.181899        -0.104448        -0.241533   \n",
       "66160        -0.095099        -0.181899        -0.104448        -0.241533   \n",
       "66161        -0.095099        -0.181899        -0.104448        -0.241533   \n",
       "\n",
       "       user_state_1016     Price  \n",
       "0            -0.128212 -0.323767  \n",
       "1            -0.128212 -0.845079  \n",
       "2            -0.128212 -0.532292  \n",
       "3            -0.128212  0.301809  \n",
       "4            -0.128212 -0.949342  \n",
       "...                ...       ...  \n",
       "66157        -0.128212  0.408156  \n",
       "66158        -0.128212 -0.322724  \n",
       "66159        -0.128212 -0.010979  \n",
       "66160        -0.128212  0.510334  \n",
       "66161        -0.128212  0.408156  \n",
       "\n",
       "[66162 rows x 2246 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns =df_updated.iloc[:,11:].columns\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delivery_Time</th>\n",
       "      <th>User_Age_years</th>\n",
       "      <th>System_Age_months</th>\n",
       "      <th>user_return_history</th>\n",
       "      <th>brand_id_return_trend</th>\n",
       "      <th>item_id_return_trend</th>\n",
       "      <th>item_size_return_trend</th>\n",
       "      <th>user_state_return_trend</th>\n",
       "      <th>item_color_return_trend</th>\n",
       "      <th>item_id_1</th>\n",
       "      <th>...</th>\n",
       "      <th>user_state_1008</th>\n",
       "      <th>user_state_1009</th>\n",
       "      <th>user_state_1010</th>\n",
       "      <th>user_state_1011</th>\n",
       "      <th>user_state_1012</th>\n",
       "      <th>user_state_1013</th>\n",
       "      <th>user_state_1014</th>\n",
       "      <th>user_state_1015</th>\n",
       "      <th>user_state_1016</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>52.0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66157</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.527277</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66158</th>\n",
       "      <td>-84.607387</td>\n",
       "      <td>35.0</td>\n",
       "      <td>59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.672222</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.510303</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66159</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.668651</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.527277</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66160</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541025</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.527277</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66161</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521088</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66162 rows × 2246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Delivery_Time  User_Age_years  System_Age_months  user_return_history  \\\n",
       "0           5.000000            52.0                 61                  0.0   \n",
       "1           5.000000            75.0                 72                  0.0   \n",
       "2           5.000000            75.0                 72                  0.0   \n",
       "3          13.000000            51.0                 76                  0.0   \n",
       "4           5.000000            51.0                 76                  0.0   \n",
       "...              ...             ...                ...                  ...   \n",
       "66157       2.000000            55.0                 77                  0.0   \n",
       "66158     -84.607387            35.0                 59                  1.0   \n",
       "66159       2.000000            55.0                 77                  0.0   \n",
       "66160       2.000000            55.0                 77                  0.0   \n",
       "66161       2.000000            55.0                 77                  0.0   \n",
       "\n",
       "       brand_id_return_trend  item_id_return_trend  item_size_return_trend  \\\n",
       "0                   0.000000              0.000000                0.000000   \n",
       "1                   0.000000              0.000000                0.000000   \n",
       "2                   0.000000              0.000000                0.000000   \n",
       "3                   0.000000              0.000000                0.000000   \n",
       "4                   0.000000              0.000000                0.000000   \n",
       "...                      ...                   ...                     ...   \n",
       "66157               1.000000              1.000000                0.527277   \n",
       "66158               0.672222              0.777778                0.666667   \n",
       "66159               0.668651              0.444444                0.527277   \n",
       "66160               0.541025              0.666667                0.527277   \n",
       "66161               1.000000              1.000000                0.521088   \n",
       "\n",
       "       user_state_return_trend  item_color_return_trend  item_id_1  ...  \\\n",
       "0                     0.000000                 0.000000          0  ...   \n",
       "1                     0.000000                 0.000000          0  ...   \n",
       "2                     0.000000                 0.000000          0  ...   \n",
       "3                     0.000000                 0.000000          0  ...   \n",
       "4                     0.000000                 0.000000          0  ...   \n",
       "...                        ...                      ...        ...  ...   \n",
       "66157                 0.355311                 0.409524          0  ...   \n",
       "66158                 0.510303                 0.409524          0  ...   \n",
       "66159                 0.355311                 1.000000          0  ...   \n",
       "66160                 0.355311                 0.333333          0  ...   \n",
       "66161                 0.355311                 0.500000          0  ...   \n",
       "\n",
       "       user_state_1008  user_state_1009  user_state_1010  user_state_1011  \\\n",
       "0                    0                0                0                0   \n",
       "1                    0                0                1                0   \n",
       "2                    0                0                1                0   \n",
       "3                    0                0                0                0   \n",
       "4                    0                0                0                0   \n",
       "...                ...              ...              ...              ...   \n",
       "66157                0                0                0                0   \n",
       "66158                0                0                0                0   \n",
       "66159                0                0                0                0   \n",
       "66160                0                0                0                0   \n",
       "66161                0                0                0                0   \n",
       "\n",
       "       user_state_1012  user_state_1013  user_state_1014  user_state_1015  \\\n",
       "0                    0                1                0                0   \n",
       "1                    0                0                0                0   \n",
       "2                    0                0                0                0   \n",
       "3                    0                0                0                0   \n",
       "4                    0                0                0                0   \n",
       "...                ...              ...              ...              ...   \n",
       "66157                0                0                0                0   \n",
       "66158                0                0                0                0   \n",
       "66159                0                0                0                0   \n",
       "66160                0                0                0                0   \n",
       "66161                0                0                0                0   \n",
       "\n",
       "       user_state_1016  Price  \n",
       "0                    0  49.90  \n",
       "1                    0  24.90  \n",
       "2                    0  39.90  \n",
       "3                    0  79.90  \n",
       "4                    0  19.90  \n",
       "...                ...    ...  \n",
       "66157                0  85.00  \n",
       "66158                0  49.95  \n",
       "66159                0  64.90  \n",
       "66160                0  89.90  \n",
       "66161                0  85.00  \n",
       "\n",
       "[66162 rows x 2246 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_updated.iloc[:,11:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into test and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_updated.columns[12:]\n",
    "\n",
    "#df_updated = df_updated.dropna()\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_updated['return'], test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the random forrest models using hyper parameter optmization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.ensemble import RandomForestClassifier'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.model_selection import GridSearchCV\\nfrom sklearn.ensemble import RandomForestClassifier'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1213) \n",
    "\n",
    "\n",
    "#name = currency_name+\"_\"+condition+\"_\"+tf\n",
    "\n",
    "param_grid = { \n",
    "'n_estimators': [500],\n",
    "'max_features': [ 'sqrt'],\n",
    "'max_depth':[15],\n",
    "'min_samples_leaf' :[2]  \n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "model =CV_rfc.fit(X_train, y_train)\n",
    "best_params = pd.DataFrame()\n",
    "best_params = CV_rfc.best_params_\n",
    "\n",
    "\n",
    "#best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.474248</td>\n",
       "      <td>0.525752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.419104</td>\n",
       "      <td>0.580896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.469496</td>\n",
       "      <td>0.530504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.553389</td>\n",
       "      <td>0.446611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.510314</td>\n",
       "      <td>0.489686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19844</th>\n",
       "      <td>0.497185</td>\n",
       "      <td>0.502815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19845</th>\n",
       "      <td>0.493184</td>\n",
       "      <td>0.506816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19846</th>\n",
       "      <td>0.490426</td>\n",
       "      <td>0.509574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19847</th>\n",
       "      <td>0.411529</td>\n",
       "      <td>0.588471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19848</th>\n",
       "      <td>0.498946</td>\n",
       "      <td>0.501054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19849 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "0      0.474248  0.525752\n",
       "1      0.419104  0.580896\n",
       "2      0.469496  0.530504\n",
       "3      0.553389  0.446611\n",
       "4      0.510314  0.489686\n",
       "...         ...       ...\n",
       "19844  0.497185  0.502815\n",
       "19845  0.493184  0.506816\n",
       "19846  0.490426  0.509574\n",
       "19847  0.411529  0.588471\n",
       "19848  0.498946  0.501054\n",
       "\n",
       "[19849 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_predict = pd.DataFrame(model.predict_proba(X_test))\n",
    "rfc_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>index</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.474248</td>\n",
       "      <td>0.525752</td>\n",
       "      <td>13618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.419104</td>\n",
       "      <td>0.580896</td>\n",
       "      <td>28860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.469496</td>\n",
       "      <td>0.530504</td>\n",
       "      <td>22085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.553389</td>\n",
       "      <td>0.446611</td>\n",
       "      <td>62991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.510314</td>\n",
       "      <td>0.489686</td>\n",
       "      <td>63544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19844</th>\n",
       "      <td>0.497185</td>\n",
       "      <td>0.502815</td>\n",
       "      <td>22024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19845</th>\n",
       "      <td>0.493184</td>\n",
       "      <td>0.506816</td>\n",
       "      <td>8027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19846</th>\n",
       "      <td>0.490426</td>\n",
       "      <td>0.509574</td>\n",
       "      <td>58102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19847</th>\n",
       "      <td>0.411529</td>\n",
       "      <td>0.588471</td>\n",
       "      <td>63296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19848</th>\n",
       "      <td>0.498946</td>\n",
       "      <td>0.501054</td>\n",
       "      <td>54797</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19849 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1  index  return\n",
       "0      0.474248  0.525752  13618       0\n",
       "1      0.419104  0.580896  28860       0\n",
       "2      0.469496  0.530504  22085       0\n",
       "3      0.553389  0.446611  62991       0\n",
       "4      0.510314  0.489686  63544       1\n",
       "...         ...       ...    ...     ...\n",
       "19844  0.497185  0.502815  22024       1\n",
       "19845  0.493184  0.506816   8027       0\n",
       "19846  0.490426  0.509574  58102       0\n",
       "19847  0.411529  0.588471  63296       1\n",
       "19848  0.498946  0.501054  54797       1\n",
       "\n",
       "[19849 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test =y_test.reset_index()\n",
    "\n",
    "final_pred = pd.concat([rfc_predict,y_test],axis =1)\n",
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred.columns =['Prob_0','Prob_1','index','return']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the predcited column in the data using probablity cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred['Predicted'] = np.where(final_pred['Prob_1']>=0.51,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19849.000000\n",
       "mean         0.505801\n",
       "std          0.055129\n",
       "min          0.224198\n",
       "25%          0.476273\n",
       "50%          0.516638\n",
       "75%          0.546013\n",
       "max          0.647995\n",
       "Name: Prob_1, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred['Prob_1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5991233815305557"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy_table =final_pred.groupby(['return']).agg(Predicted_mean=('Predicted', 'mean'),Predicted_1=('Predicted', 'sum'),Predicted_0=('Predicted', 'count'))\n",
    "Accuracy_table= Accuracy_table.reset_index()\n",
    "Accuracy_table.columns = ['Actual_return','predicted_mean','predicted_1','predicted_all']\n",
    "Accuracy_table['predicted_0'] = Accuracy_table['predicted_all'] - Accuracy_table['predicted_1']\n",
    "Accuracy = (Accuracy_table['predicted_0'][0] + Accuracy_table['predicted_1'][1] )/Accuracy_table['predicted_all'].sum()\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the confusion matrix table to asses the model perfromance and create lift chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p_table =pd.DataFrame()\n",
    "\n",
    "list_a = [10,20,30,40,50,60,70,80,90]\n",
    "\n",
    "i=0\n",
    "\n",
    "for n in list_a:\n",
    "           \n",
    "\n",
    "            p_table.loc[i,'Percentile'] = n\n",
    "\n",
    "            p_table.loc[i,'Cut_Off_Probability'] = np.percentile(np.array(final_pred['Prob_1']),n)\n",
    "\n",
    "            cut_off = p_table.loc[i,'Cut_Off_Probability']\n",
    "\n",
    "            p_table.loc[i,'TN'] = len(final_pred[final_pred['return'] == 0]\\\n",
    "                                      [final_pred['Prob_1'] < cut_off])\n",
    "\n",
    "            p_table.loc[i,'FP'] = len(final_pred[final_pred['return'] == 0]\\\n",
    "                                      [final_pred['Prob_1'] >= cut_off])\n",
    "\n",
    "            p_table.loc[i,'FN'] = len(final_pred[final_pred['return'] == 1]\\\n",
    "                                      [final_pred['Prob_1'] < cut_off])\n",
    "\n",
    "            p_table.loc[i,'TP'] = len(final_pred[final_pred['return'] == 1]\\\n",
    "                                      [final_pred['Prob_1'] >= cut_off])\n",
    "\n",
    "            p_table.loc[i,'Total'] = p_table.loc[i,'TN'] + p_table.loc[i,'FP'] + p_table.loc[i,'FN'] + p_table.loc[i,'TP']\n",
    "\n",
    "            p_table.loc[i,'FP+TP'] = p_table.loc[i,'FP'] + p_table.loc[i,'TP']\n",
    "\n",
    "        \n",
    "\n",
    "            p_table.loc[i,'Predicted 1 Accuracy'] = p_table.loc[i,'TP']/(p_table.loc[i,'TP'] + p_table.loc[i,'FP'])\n",
    "\n",
    "          \n",
    "\n",
    "            i+=1\n",
    "\n",
    "\n",
    "\n",
    "p_table.to_csv(\"Lift_chart_RF_0706V1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the relative importance of variables in the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from rfpimp import permutation_importances\n",
    "\n",
    "def r2(model, X_train, y_train):\n",
    "    return r2_score(y_train, model.predict(X_train))\n",
    "\n",
    "perm_imp_rfpimp = permutation_importances(model, X_train, y_train, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_imp_rfpimp.to_csv('relative_importance_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Building the Neural Network models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1) \n",
    "\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(3)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import clear_output\n",
    "from numpy import arange\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, LeakyReLU, BatchNormalization, Dropout\n",
    "from keras.activations import relu, sigmoid\n",
    "import random \n",
    "import time\n",
    "from keras.layers import LeakyReLU\n",
    "import h5py\n",
    "\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "#import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating a 3 hidden layer NN models with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.6844 - accuracy: 0.5585\n",
      "Epoch 2/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.6673 - accuracy: 0.6008\n",
      "Epoch 3/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6596 - accuracy: 0.6083\n",
      "Epoch 4/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.6541 - accuracy: 0.6157\n",
      "Epoch 5/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6499 - accuracy: 0.6219\n",
      "Epoch 6/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.6463 - accuracy: 0.6268\n",
      "Epoch 7/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6423 - accuracy: 0.6327\n",
      "Epoch 8/1000\n",
      "464/464 [==============================] - 4s 8ms/step - loss: 0.6387 - accuracy: 0.6385\n",
      "Epoch 9/1000\n",
      "464/464 [==============================] - 6s 12ms/step - loss: 0.6353 - accuracy: 0.6413 1s - loss: 0.6341 - accuracy\n",
      "Epoch 10/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.6317 - accuracy: 0.6438\n",
      "Epoch 11/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6286 - accuracy: 0.6491\n",
      "Epoch 12/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6256 - accuracy: 0.6511\n",
      "Epoch 13/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.6228 - accuracy: 0.6531\n",
      "Epoch 14/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6199 - accuracy: 0.6553\n",
      "Epoch 15/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6174 - accuracy: 0.6583\n",
      "Epoch 16/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6153 - accuracy: 0.6601\n",
      "Epoch 17/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6135 - accuracy: 0.6600\n",
      "Epoch 18/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6111 - accuracy: 0.6628\n",
      "Epoch 19/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6096 - accuracy: 0.6708\n",
      "Epoch 20/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.6076 - accuracy: 0.6760\n",
      "Epoch 21/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.6062 - accuracy: 0.6766\n",
      "Epoch 22/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.6044 - accuracy: 0.6780\n",
      "Epoch 23/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6028 - accuracy: 0.6800\n",
      "Epoch 24/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.6015 - accuracy: 0.6817\n",
      "Epoch 25/1000\n",
      "464/464 [==============================] - ETA: 0s - loss: 0.6002 - accuracy: 0.68 - 2s 5ms/step - loss: 0.6005 - accuracy: 0.6821\n",
      "Epoch 26/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5989 - accuracy: 0.6842\n",
      "Epoch 27/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5971 - accuracy: 0.6843\n",
      "Epoch 28/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5965 - accuracy: 0.6867\n",
      "Epoch 29/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5956 - accuracy: 0.6876\n",
      "Epoch 30/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5942 - accuracy: 0.6872\n",
      "Epoch 31/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5928 - accuracy: 0.6908\n",
      "Epoch 32/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5919 - accuracy: 0.6909\n",
      "Epoch 33/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5908 - accuracy: 0.6915\n",
      "Epoch 34/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5893 - accuracy: 0.6939\n",
      "Epoch 35/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5883 - accuracy: 0.6941\n",
      "Epoch 36/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5877 - accuracy: 0.6946\n",
      "Epoch 37/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5868 - accuracy: 0.6957\n",
      "Epoch 38/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5857 - accuracy: 0.6970\n",
      "Epoch 39/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5853 - accuracy: 0.6968\n",
      "Epoch 40/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5844 - accuracy: 0.6985\n",
      "Epoch 41/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5828 - accuracy: 0.7005\n",
      "Epoch 42/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5826 - accuracy: 0.7006\n",
      "Epoch 43/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5817 - accuracy: 0.7005\n",
      "Epoch 44/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5816 - accuracy: 0.7009\n",
      "Epoch 45/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5802 - accuracy: 0.7015\n",
      "Epoch 46/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5797 - accuracy: 0.7021\n",
      "Epoch 47/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5786 - accuracy: 0.7036\n",
      "Epoch 48/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5780 - accuracy: 0.7036\n",
      "Epoch 49/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5773 - accuracy: 0.7039\n",
      "Epoch 50/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5769 - accuracy: 0.7040\n",
      "Epoch 51/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5766 - accuracy: 0.7051\n",
      "Epoch 52/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5756 - accuracy: 0.7059\n",
      "Epoch 53/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5749 - accuracy: 0.7057\n",
      "Epoch 54/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5750 - accuracy: 0.7065\n",
      "Epoch 55/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5735 - accuracy: 0.7073\n",
      "Epoch 56/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5737 - accuracy: 0.7062\n",
      "Epoch 57/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5725 - accuracy: 0.7078\n",
      "Epoch 58/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5722 - accuracy: 0.7077\n",
      "Epoch 59/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5716 - accuracy: 0.7089\n",
      "Epoch 60/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5715 - accuracy: 0.7084\n",
      "Epoch 61/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5704 - accuracy: 0.7097\n",
      "Epoch 62/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5707 - accuracy: 0.7086\n",
      "Epoch 63/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5697 - accuracy: 0.7096\n",
      "Epoch 64/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5688 - accuracy: 0.7112\n",
      "Epoch 65/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5684 - accuracy: 0.7117\n",
      "Epoch 66/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5680 - accuracy: 0.7125\n",
      "Epoch 67/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5678 - accuracy: 0.7114\n",
      "Epoch 68/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5673 - accuracy: 0.7121\n",
      "Epoch 69/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5674 - accuracy: 0.7130\n",
      "Epoch 70/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5667 - accuracy: 0.7125\n",
      "Epoch 71/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5658 - accuracy: 0.7140\n",
      "Epoch 72/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5650 - accuracy: 0.7142\n",
      "Epoch 73/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5648 - accuracy: 0.7146\n",
      "Epoch 74/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5645 - accuracy: 0.7144\n",
      "Epoch 75/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5646 - accuracy: 0.7144\n",
      "Epoch 76/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5640 - accuracy: 0.7152\n",
      "Epoch 77/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5634 - accuracy: 0.7145\n",
      "Epoch 78/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5636 - accuracy: 0.7150\n",
      "Epoch 79/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5637 - accuracy: 0.7149\n",
      "Epoch 80/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5627 - accuracy: 0.7158\n",
      "Epoch 81/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5624 - accuracy: 0.7167\n",
      "Epoch 82/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5617 - accuracy: 0.7168\n",
      "Epoch 83/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5621 - accuracy: 0.7155\n",
      "Epoch 84/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5608 - accuracy: 0.7173\n",
      "Epoch 85/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5601 - accuracy: 0.7183\n",
      "Epoch 86/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5600 - accuracy: 0.7191\n",
      "Epoch 87/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5601 - accuracy: 0.7183\n",
      "Epoch 88/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5606 - accuracy: 0.7177\n",
      "Epoch 89/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5605 - accuracy: 0.7174\n",
      "Epoch 90/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5598 - accuracy: 0.7179\n",
      "Epoch 91/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5597 - accuracy: 0.7186\n",
      "Epoch 92/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5587 - accuracy: 0.7189\n",
      "Epoch 93/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5591 - accuracy: 0.7192\n",
      "Epoch 94/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5584 - accuracy: 0.7199\n",
      "Epoch 95/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5582 - accuracy: 0.7205\n",
      "Epoch 96/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5570 - accuracy: 0.7206\n",
      "Epoch 97/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5570 - accuracy: 0.7206\n",
      "Epoch 98/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5573 - accuracy: 0.7212\n",
      "Epoch 99/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5567 - accuracy: 0.7212\n",
      "Epoch 100/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5566 - accuracy: 0.7210\n",
      "Epoch 101/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5567 - accuracy: 0.7207\n",
      "Epoch 102/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5571 - accuracy: 0.7203\n",
      "Epoch 103/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5567 - accuracy: 0.7211\n",
      "Epoch 104/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5563 - accuracy: 0.7223\n",
      "Epoch 105/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5555 - accuracy: 0.7218\n",
      "Epoch 106/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5551 - accuracy: 0.7221\n",
      "Epoch 107/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5563 - accuracy: 0.7214\n",
      "Epoch 108/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5561 - accuracy: 0.7211\n",
      "Epoch 109/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5556 - accuracy: 0.7221\n",
      "Epoch 110/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5551 - accuracy: 0.7224\n",
      "Epoch 111/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5555 - accuracy: 0.7229\n",
      "Epoch 112/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5541 - accuracy: 0.7228\n",
      "Epoch 113/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5532 - accuracy: 0.7248\n",
      "Epoch 114/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5543 - accuracy: 0.7232\n",
      "Epoch 115/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5537 - accuracy: 0.7239\n",
      "Epoch 116/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5546 - accuracy: 0.7233\n",
      "Epoch 117/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5534 - accuracy: 0.7233\n",
      "Epoch 118/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5532 - accuracy: 0.7228\n",
      "Epoch 119/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5534 - accuracy: 0.7228\n",
      "Epoch 120/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5533 - accuracy: 0.7227\n",
      "Epoch 121/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5527 - accuracy: 0.7241\n",
      "Epoch 122/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5528 - accuracy: 0.7247\n",
      "Epoch 123/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5533 - accuracy: 0.7249\n",
      "Epoch 124/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5523 - accuracy: 0.7246\n",
      "Epoch 125/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5522 - accuracy: 0.7240\n",
      "Epoch 126/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5521 - accuracy: 0.7244\n",
      "Epoch 127/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5520 - accuracy: 0.7244\n",
      "Epoch 128/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5522 - accuracy: 0.7247\n",
      "Epoch 129/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5518 - accuracy: 0.7245\n",
      "Epoch 130/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5523 - accuracy: 0.7245\n",
      "Epoch 131/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5522 - accuracy: 0.7247\n",
      "Epoch 132/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5515 - accuracy: 0.7259\n",
      "Epoch 133/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5513 - accuracy: 0.7263\n",
      "Epoch 134/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5508 - accuracy: 0.7253\n",
      "Epoch 135/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5514 - accuracy: 0.7253\n",
      "Epoch 136/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5504 - accuracy: 0.7247\n",
      "Epoch 137/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5512 - accuracy: 0.7250\n",
      "Epoch 138/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5506 - accuracy: 0.7258\n",
      "Epoch 139/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5504 - accuracy: 0.7254\n",
      "Epoch 140/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5497 - accuracy: 0.7268\n",
      "Epoch 141/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5491 - accuracy: 0.7270\n",
      "Epoch 142/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5492 - accuracy: 0.7257\n",
      "Epoch 143/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5495 - accuracy: 0.7266\n",
      "Epoch 144/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5499 - accuracy: 0.7259\n",
      "Epoch 145/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5503 - accuracy: 0.7266\n",
      "Epoch 146/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5501 - accuracy: 0.7263\n",
      "Epoch 147/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5499 - accuracy: 0.7265\n",
      "Epoch 148/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5489 - accuracy: 0.7272\n",
      "Epoch 149/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5490 - accuracy: 0.7271\n",
      "Epoch 150/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5501 - accuracy: 0.7261\n",
      "Epoch 151/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5489 - accuracy: 0.7286\n",
      "Epoch 152/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5491 - accuracy: 0.7267\n",
      "Epoch 153/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5488 - accuracy: 0.7270\n",
      "Epoch 154/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5483 - accuracy: 0.7274\n",
      "Epoch 155/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5485 - accuracy: 0.7274\n",
      "Epoch 156/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5488 - accuracy: 0.7269\n",
      "Epoch 157/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5481 - accuracy: 0.7282\n",
      "Epoch 158/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5481 - accuracy: 0.7279\n",
      "Epoch 159/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5478 - accuracy: 0.7280\n",
      "Epoch 160/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5480 - accuracy: 0.7278\n",
      "Epoch 161/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5475 - accuracy: 0.7283\n",
      "Epoch 162/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5473 - accuracy: 0.7290\n",
      "Epoch 163/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5470 - accuracy: 0.7283\n",
      "Epoch 164/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5474 - accuracy: 0.7285\n",
      "Epoch 165/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5474 - accuracy: 0.7285\n",
      "Epoch 166/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5474 - accuracy: 0.7282\n",
      "Epoch 167/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5473 - accuracy: 0.7283\n",
      "Epoch 168/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5464 - accuracy: 0.7288\n",
      "Epoch 169/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5467 - accuracy: 0.7290\n",
      "Epoch 170/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5472 - accuracy: 0.7282\n",
      "Epoch 171/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5463 - accuracy: 0.7291\n",
      "Epoch 172/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5471 - accuracy: 0.7285\n",
      "Epoch 173/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5467 - accuracy: 0.7284\n",
      "Epoch 174/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5460 - accuracy: 0.7292\n",
      "Epoch 175/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5459 - accuracy: 0.7294\n",
      "Epoch 176/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5460 - accuracy: 0.7292\n",
      "Epoch 177/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5455 - accuracy: 0.7301\n",
      "Epoch 178/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5456 - accuracy: 0.7296\n",
      "Epoch 179/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5455 - accuracy: 0.7294\n",
      "Epoch 180/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5456 - accuracy: 0.7303\n",
      "Epoch 181/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5453 - accuracy: 0.7298\n",
      "Epoch 182/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5445 - accuracy: 0.7308\n",
      "Epoch 183/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5451 - accuracy: 0.7296\n",
      "Epoch 184/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5455 - accuracy: 0.7294\n",
      "Epoch 185/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5455 - accuracy: 0.7300\n",
      "Epoch 186/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5461 - accuracy: 0.7295\n",
      "Epoch 187/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5451 - accuracy: 0.7299\n",
      "Epoch 188/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5447 - accuracy: 0.7304\n",
      "Epoch 189/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5450 - accuracy: 0.7295\n",
      "Epoch 190/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5444 - accuracy: 0.7308\n",
      "Epoch 191/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5444 - accuracy: 0.7312\n",
      "Epoch 192/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5440 - accuracy: 0.7306\n",
      "Epoch 193/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5447 - accuracy: 0.7308\n",
      "Epoch 194/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5451 - accuracy: 0.7296\n",
      "Epoch 195/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5439 - accuracy: 0.7305\n",
      "Epoch 196/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5448 - accuracy: 0.7302\n",
      "Epoch 197/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5437 - accuracy: 0.7322\n",
      "Epoch 198/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5434 - accuracy: 0.7313\n",
      "Epoch 199/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5440 - accuracy: 0.7301\n",
      "Epoch 200/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5437 - accuracy: 0.7320\n",
      "Epoch 201/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5432 - accuracy: 0.7316\n",
      "Epoch 202/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5436 - accuracy: 0.7310\n",
      "Epoch 203/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5435 - accuracy: 0.7309\n",
      "Epoch 204/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5431 - accuracy: 0.7309\n",
      "Epoch 205/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5431 - accuracy: 0.7317\n",
      "Epoch 206/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5438 - accuracy: 0.7308\n",
      "Epoch 207/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5433 - accuracy: 0.7312\n",
      "Epoch 208/1000\n",
      "464/464 [==============================] - ETA: 0s - loss: 0.5420 - accuracy: 0.73 - 2s 3ms/step - loss: 0.5425 - accuracy: 0.7317\n",
      "Epoch 209/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5426 - accuracy: 0.7324\n",
      "Epoch 210/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5423 - accuracy: 0.7323\n",
      "Epoch 211/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5426 - accuracy: 0.7317\n",
      "Epoch 212/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5430 - accuracy: 0.7320\n",
      "Epoch 213/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5427 - accuracy: 0.7326\n",
      "Epoch 214/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5431 - accuracy: 0.7321\n",
      "Epoch 215/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5427 - accuracy: 0.7315\n",
      "Epoch 216/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5419 - accuracy: 0.7322\n",
      "Epoch 217/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5433 - accuracy: 0.7315\n",
      "Epoch 218/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5420 - accuracy: 0.7324\n",
      "Epoch 219/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5415 - accuracy: 0.7327\n",
      "Epoch 220/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5424 - accuracy: 0.7323\n",
      "Epoch 221/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5419 - accuracy: 0.7325\n",
      "Epoch 222/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5421 - accuracy: 0.7326\n",
      "Epoch 223/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5420 - accuracy: 0.7323\n",
      "Epoch 224/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5431 - accuracy: 0.7315\n",
      "Epoch 225/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5421 - accuracy: 0.7321\n",
      "Epoch 226/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5427 - accuracy: 0.7316\n",
      "Epoch 227/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5431 - accuracy: 0.7320\n",
      "Epoch 228/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5420 - accuracy: 0.7323\n",
      "Epoch 229/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5421 - accuracy: 0.7321\n",
      "Epoch 230/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5414 - accuracy: 0.7322\n",
      "Epoch 231/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5419 - accuracy: 0.7330\n",
      "Epoch 232/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5425 - accuracy: 0.7317\n",
      "Epoch 233/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5418 - accuracy: 0.7327\n",
      "Epoch 234/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5413 - accuracy: 0.7336\n",
      "Epoch 235/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5426 - accuracy: 0.7322\n",
      "Epoch 236/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5417 - accuracy: 0.7330\n",
      "Epoch 237/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5406 - accuracy: 0.7334\n",
      "Epoch 238/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5420 - accuracy: 0.7320\n",
      "Epoch 239/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5409 - accuracy: 0.7331\n",
      "Epoch 240/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5410 - accuracy: 0.7330\n",
      "Epoch 241/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5402 - accuracy: 0.7335\n",
      "Epoch 242/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5411 - accuracy: 0.7325\n",
      "Epoch 243/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5406 - accuracy: 0.7331\n",
      "Epoch 244/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5416 - accuracy: 0.7323\n",
      "Epoch 245/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5403 - accuracy: 0.7333\n",
      "Epoch 246/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5422 - accuracy: 0.7327\n",
      "Epoch 247/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5403 - accuracy: 0.7337\n",
      "Epoch 248/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5400 - accuracy: 0.7347\n",
      "Epoch 249/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5401 - accuracy: 0.7337\n",
      "Epoch 250/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5400 - accuracy: 0.7338\n",
      "Epoch 251/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5408 - accuracy: 0.7330\n",
      "Epoch 252/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5411 - accuracy: 0.7332\n",
      "Epoch 253/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5408 - accuracy: 0.7323\n",
      "Epoch 254/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5400 - accuracy: 0.7334\n",
      "Epoch 255/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5406 - accuracy: 0.7340\n",
      "Epoch 256/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5405 - accuracy: 0.7322\n",
      "Epoch 257/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5397 - accuracy: 0.7338\n",
      "Epoch 258/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7338\n",
      "Epoch 259/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5403 - accuracy: 0.7335\n",
      "Epoch 260/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5397 - accuracy: 0.7330\n",
      "Epoch 261/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5399 - accuracy: 0.7336\n",
      "Epoch 262/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5401 - accuracy: 0.7331\n",
      "Epoch 263/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5401 - accuracy: 0.7334\n",
      "Epoch 264/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5388 - accuracy: 0.7344\n",
      "Epoch 265/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5399 - accuracy: 0.7338\n",
      "Epoch 266/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5393 - accuracy: 0.7345\n",
      "Epoch 267/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5396 - accuracy: 0.7344\n",
      "Epoch 268/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5397 - accuracy: 0.7340\n",
      "Epoch 269/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5389 - accuracy: 0.7342\n",
      "Epoch 270/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5391 - accuracy: 0.7339\n",
      "Epoch 271/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5390 - accuracy: 0.7338\n",
      "Epoch 272/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5395 - accuracy: 0.7340\n",
      "Epoch 273/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5398 - accuracy: 0.7333\n",
      "Epoch 274/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5394 - accuracy: 0.7332\n",
      "Epoch 275/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5392 - accuracy: 0.7339\n",
      "Epoch 276/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5401 - accuracy: 0.7335\n",
      "Epoch 277/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5393 - accuracy: 0.7334\n",
      "Epoch 278/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5395 - accuracy: 0.7331\n",
      "Epoch 279/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5397 - accuracy: 0.7328\n",
      "Epoch 280/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5397 - accuracy: 0.7326\n",
      "Epoch 281/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5389 - accuracy: 0.7338\n",
      "Epoch 282/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5398 - accuracy: 0.7340\n",
      "Epoch 283/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5392 - accuracy: 0.7339\n",
      "Epoch 284/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5394 - accuracy: 0.7330\n",
      "Epoch 285/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5394 - accuracy: 0.7348\n",
      "Epoch 286/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5392 - accuracy: 0.7342\n",
      "Epoch 287/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5388 - accuracy: 0.7347\n",
      "Epoch 288/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5385 - accuracy: 0.7351\n",
      "Epoch 289/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5386 - accuracy: 0.7342\n",
      "Epoch 290/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5388 - accuracy: 0.7340\n",
      "Epoch 291/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5381 - accuracy: 0.7352\n",
      "Epoch 292/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5381 - accuracy: 0.7353\n",
      "Epoch 293/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5382 - accuracy: 0.7352\n",
      "Epoch 294/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5389 - accuracy: 0.7347\n",
      "Epoch 295/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5383 - accuracy: 0.7340\n",
      "Epoch 296/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5391 - accuracy: 0.7343\n",
      "Epoch 297/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5390 - accuracy: 0.7339\n",
      "Epoch 298/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5382 - accuracy: 0.7344\n",
      "Epoch 299/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5388 - accuracy: 0.7341\n",
      "Epoch 300/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5383 - accuracy: 0.7357\n",
      "Epoch 301/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5382 - accuracy: 0.7351\n",
      "Epoch 302/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5378 - accuracy: 0.7347\n",
      "Epoch 303/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5380 - accuracy: 0.7345\n",
      "Epoch 304/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5376 - accuracy: 0.7358\n",
      "Epoch 305/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5373 - accuracy: 0.7351\n",
      "Epoch 306/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5374 - accuracy: 0.7358\n",
      "Epoch 307/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5377 - accuracy: 0.7356\n",
      "Epoch 308/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5371 - accuracy: 0.7355\n",
      "Epoch 309/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5378 - accuracy: 0.7354\n",
      "Epoch 310/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5364 - accuracy: 0.7360\n",
      "Epoch 311/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5377 - accuracy: 0.7355\n",
      "Epoch 312/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5379 - accuracy: 0.7355\n",
      "Epoch 313/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5384 - accuracy: 0.7357\n",
      "Epoch 314/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5375 - accuracy: 0.7358\n",
      "Epoch 315/1000\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 0.5369 - accuracy: 0.7358\n",
      "Epoch 316/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5364 - accuracy: 0.7355\n",
      "Epoch 317/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5370 - accuracy: 0.7361\n",
      "Epoch 318/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5373 - accuracy: 0.7358\n",
      "Epoch 319/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5377 - accuracy: 0.7355\n",
      "Epoch 320/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5375 - accuracy: 0.7366\n",
      "Epoch 321/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5372 - accuracy: 0.7356\n",
      "Epoch 322/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5383 - accuracy: 0.7353\n",
      "Epoch 323/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5377 - accuracy: 0.7350\n",
      "Epoch 324/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5374 - accuracy: 0.7352\n",
      "Epoch 325/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5368 - accuracy: 0.7364\n",
      "Epoch 326/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5377 - accuracy: 0.7357\n",
      "Epoch 327/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5371 - accuracy: 0.7359\n",
      "Epoch 328/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5369 - accuracy: 0.7360\n",
      "Epoch 329/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5372 - accuracy: 0.7362\n",
      "Epoch 330/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5381 - accuracy: 0.7349\n",
      "Epoch 331/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5375 - accuracy: 0.7356\n",
      "Epoch 332/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5366 - accuracy: 0.7371\n",
      "Epoch 333/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5361 - accuracy: 0.7375\n",
      "Epoch 334/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5368 - accuracy: 0.7361\n",
      "Epoch 335/1000\n",
      "464/464 [==============================] - 5s 10ms/step - loss: 0.5375 - accuracy: 0.7364\n",
      "Epoch 336/1000\n",
      "464/464 [==============================] - 5s 10ms/step - loss: 0.5368 - accuracy: 0.7363\n",
      "Epoch 337/1000\n",
      "464/464 [==============================] - 5s 10ms/step - loss: 0.5362 - accuracy: 0.7364\n",
      "Epoch 338/1000\n",
      "464/464 [==============================] - 5s 10ms/step - loss: 0.5373 - accuracy: 0.7349\n",
      "Epoch 339/1000\n",
      "464/464 [==============================] - 5s 11ms/step - loss: 0.5374 - accuracy: 0.7360\n",
      "Epoch 340/1000\n",
      "464/464 [==============================] - 5s 10ms/step - loss: 0.5366 - accuracy: 0.7368\n",
      "Epoch 341/1000\n",
      "464/464 [==============================] - 5s 10ms/step - loss: 0.5364 - accuracy: 0.7372\n",
      "Epoch 342/1000\n",
      "464/464 [==============================] - 5s 11ms/step - loss: 0.5366 - accuracy: 0.7362\n",
      "Epoch 343/1000\n",
      "464/464 [==============================] - 5s 10ms/step - loss: 0.5361 - accuracy: 0.7367\n",
      "Epoch 344/1000\n",
      "464/464 [==============================] - 4s 9ms/step - loss: 0.5363 - accuracy: 0.7358\n",
      "Epoch 345/1000\n",
      "464/464 [==============================] - 5s 10ms/step - loss: 0.5357 - accuracy: 0.7368\n",
      "Epoch 346/1000\n",
      "464/464 [==============================] - 4s 9ms/step - loss: 0.5367 - accuracy: 0.7370\n",
      "Epoch 347/1000\n",
      "464/464 [==============================] - 4s 9ms/step - loss: 0.5360 - accuracy: 0.7367\n",
      "Epoch 348/1000\n",
      "464/464 [==============================] - 4s 8ms/step - loss: 0.5364 - accuracy: 0.7359\n",
      "Epoch 349/1000\n",
      "464/464 [==============================] - 4s 8ms/step - loss: 0.5363 - accuracy: 0.7356\n",
      "Epoch 350/1000\n",
      "464/464 [==============================] - 4s 9ms/step - loss: 0.5368 - accuracy: 0.7360\n",
      "Epoch 351/1000\n",
      "464/464 [==============================] - 4s 8ms/step - loss: 0.5361 - accuracy: 0.7363\n",
      "Epoch 352/1000\n",
      "464/464 [==============================] - 4s 8ms/step - loss: 0.5351 - accuracy: 0.7373\n",
      "Epoch 353/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5360 - accuracy: 0.7359\n",
      "Epoch 354/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5361 - accuracy: 0.7370\n",
      "Epoch 355/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5362 - accuracy: 0.7370\n",
      "Epoch 356/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5369 - accuracy: 0.7354\n",
      "Epoch 357/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5348 - accuracy: 0.7378\n",
      "Epoch 358/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5358 - accuracy: 0.7375\n",
      "Epoch 359/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5366 - accuracy: 0.7368\n",
      "Epoch 360/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5356 - accuracy: 0.7371\n",
      "Epoch 361/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5364 - accuracy: 0.7372\n",
      "Epoch 362/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5358 - accuracy: 0.7365\n",
      "Epoch 363/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5355 - accuracy: 0.7373\n",
      "Epoch 364/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5354 - accuracy: 0.7374\n",
      "Epoch 365/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5359 - accuracy: 0.7368\n",
      "Epoch 366/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5362 - accuracy: 0.7361\n",
      "Epoch 367/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5358 - accuracy: 0.7371\n",
      "Epoch 368/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5357 - accuracy: 0.7371\n",
      "Epoch 369/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5353 - accuracy: 0.7383\n",
      "Epoch 370/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5358 - accuracy: 0.7374\n",
      "Epoch 371/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5354 - accuracy: 0.7371\n",
      "Epoch 372/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5356 - accuracy: 0.7376\n",
      "Epoch 373/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5363 - accuracy: 0.7367\n",
      "Epoch 374/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5352 - accuracy: 0.7371\n",
      "Epoch 375/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5354 - accuracy: 0.7377\n",
      "Epoch 376/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5358 - accuracy: 0.7367\n",
      "Epoch 377/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5347 - accuracy: 0.7377\n",
      "Epoch 378/1000\n",
      "464/464 [==============================] - 4s 8ms/step - loss: 0.5359 - accuracy: 0.7369\n",
      "Epoch 379/1000\n",
      "464/464 [==============================] - 4s 8ms/step - loss: 0.5347 - accuracy: 0.7383\n",
      "Epoch 380/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5348 - accuracy: 0.7375\n",
      "Epoch 381/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5360 - accuracy: 0.7383\n",
      "Epoch 382/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5354 - accuracy: 0.7373\n",
      "Epoch 383/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5348 - accuracy: 0.7385\n",
      "Epoch 384/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5347 - accuracy: 0.7382\n",
      "Epoch 385/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5345 - accuracy: 0.7383\n",
      "Epoch 386/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5349 - accuracy: 0.7370\n",
      "Epoch 387/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5363 - accuracy: 0.7366\n",
      "Epoch 388/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5352 - accuracy: 0.7380\n",
      "Epoch 389/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5353 - accuracy: 0.7375\n",
      "Epoch 390/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5358 - accuracy: 0.7376\n",
      "Epoch 391/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5351 - accuracy: 0.7379\n",
      "Epoch 392/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5350 - accuracy: 0.7371\n",
      "Epoch 393/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5349 - accuracy: 0.7372\n",
      "Epoch 394/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5348 - accuracy: 0.7381\n",
      "Epoch 395/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5349 - accuracy: 0.7378\n",
      "Epoch 396/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5354 - accuracy: 0.7379\n",
      "Epoch 397/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5354 - accuracy: 0.7371\n",
      "Epoch 398/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5358 - accuracy: 0.7378\n",
      "Epoch 399/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5351 - accuracy: 0.7379\n",
      "Epoch 400/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5345 - accuracy: 0.7379\n",
      "Epoch 401/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5347 - accuracy: 0.7380\n",
      "Epoch 402/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5350 - accuracy: 0.7384\n",
      "Epoch 403/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5349 - accuracy: 0.7383\n",
      "Epoch 404/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5360 - accuracy: 0.7370\n",
      "Epoch 405/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5354 - accuracy: 0.7388\n",
      "Epoch 406/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5353 - accuracy: 0.7378\n",
      "Epoch 407/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5345 - accuracy: 0.7383\n",
      "Epoch 408/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5343 - accuracy: 0.7390\n",
      "Epoch 409/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5338 - accuracy: 0.7394\n",
      "Epoch 410/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5351 - accuracy: 0.7381\n",
      "Epoch 411/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5349 - accuracy: 0.7384\n",
      "Epoch 412/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5344 - accuracy: 0.7389\n",
      "Epoch 413/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5343 - accuracy: 0.7388\n",
      "Epoch 414/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5349 - accuracy: 0.7379\n",
      "Epoch 415/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5349 - accuracy: 0.7388\n",
      "Epoch 416/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5344 - accuracy: 0.7396\n",
      "Epoch 417/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5352 - accuracy: 0.7382\n",
      "Epoch 418/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5350 - accuracy: 0.7381\n",
      "Epoch 419/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5339 - accuracy: 0.7388\n",
      "Epoch 420/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5344 - accuracy: 0.7390\n",
      "Epoch 421/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5336 - accuracy: 0.7385\n",
      "Epoch 422/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5347 - accuracy: 0.7379\n",
      "Epoch 423/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5352 - accuracy: 0.7366\n",
      "Epoch 424/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5346 - accuracy: 0.7376\n",
      "Epoch 425/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5349 - accuracy: 0.7379\n",
      "Epoch 426/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5342 - accuracy: 0.7373\n",
      "Epoch 427/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5358 - accuracy: 0.7373\n",
      "Epoch 428/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5336 - accuracy: 0.7385\n",
      "Epoch 429/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5336 - accuracy: 0.7393\n",
      "Epoch 430/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5344 - accuracy: 0.7379\n",
      "Epoch 431/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5339 - accuracy: 0.7400\n",
      "Epoch 432/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5341 - accuracy: 0.7387\n",
      "Epoch 433/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5344 - accuracy: 0.7386\n",
      "Epoch 434/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5340 - accuracy: 0.7382\n",
      "Epoch 435/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5341 - accuracy: 0.7382\n",
      "Epoch 436/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5339 - accuracy: 0.7377\n",
      "Epoch 437/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5349 - accuracy: 0.7387\n",
      "Epoch 438/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5346 - accuracy: 0.7386\n",
      "Epoch 439/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5355 - accuracy: 0.7366\n",
      "Epoch 440/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5343 - accuracy: 0.7390\n",
      "Epoch 441/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5340 - accuracy: 0.7391\n",
      "Epoch 442/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5340 - accuracy: 0.7388\n",
      "Epoch 443/1000\n",
      "464/464 [==============================] - 4s 8ms/step - loss: 0.5349 - accuracy: 0.7384\n",
      "Epoch 444/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5344 - accuracy: 0.7380\n",
      "Epoch 445/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5343 - accuracy: 0.7382\n",
      "Epoch 446/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5343 - accuracy: 0.7377\n",
      "Epoch 447/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5330 - accuracy: 0.7390\n",
      "Epoch 448/1000\n",
      "464/464 [==============================] - 2s 3ms/step - loss: 0.5334 - accuracy: 0.7391\n",
      "Epoch 449/1000\n",
      "464/464 [==============================] - ETA: 0s - loss: 0.5335 - accuracy: 0.73 - 2s 3ms/step - loss: 0.5336 - accuracy: 0.7393\n",
      "Epoch 450/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5341 - accuracy: 0.7382\n",
      "Epoch 451/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5345 - accuracy: 0.7379\n",
      "Epoch 452/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5341 - accuracy: 0.7390\n",
      "Epoch 453/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5340 - accuracy: 0.7391\n",
      "Epoch 454/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5337 - accuracy: 0.7383\n",
      "Epoch 455/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5331 - accuracy: 0.7397\n",
      "Epoch 456/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5342 - accuracy: 0.7388\n",
      "Epoch 457/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5333 - accuracy: 0.7391\n",
      "Epoch 458/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5336 - accuracy: 0.7391\n",
      "Epoch 459/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5343 - accuracy: 0.7377\n",
      "Epoch 460/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5332 - accuracy: 0.7402\n",
      "Epoch 461/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5331 - accuracy: 0.7401\n",
      "Epoch 462/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5334 - accuracy: 0.7397\n",
      "Epoch 463/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5338 - accuracy: 0.7386\n",
      "Epoch 464/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5344 - accuracy: 0.7386\n",
      "Epoch 465/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5337 - accuracy: 0.7390\n",
      "Epoch 466/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5330 - accuracy: 0.7391\n",
      "Epoch 467/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5332 - accuracy: 0.7392\n",
      "Epoch 468/1000\n",
      "464/464 [==============================] - 3s 5ms/step - loss: 0.5331 - accuracy: 0.7393\n",
      "Epoch 469/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5339 - accuracy: 0.7381\n",
      "Epoch 470/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464/464 [==============================] - 4s 8ms/step - loss: 0.5341 - accuracy: 0.7398\n",
      "Epoch 471/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5329 - accuracy: 0.7396\n",
      "Epoch 472/1000\n",
      "464/464 [==============================] - 4s 9ms/step - loss: 0.5334 - accuracy: 0.7394\n",
      "Epoch 473/1000\n",
      "464/464 [==============================] - 4s 9ms/step - loss: 0.5334 - accuracy: 0.7391\n",
      "Epoch 474/1000\n",
      "464/464 [==============================] - 4s 8ms/step - loss: 0.5334 - accuracy: 0.7399\n",
      "Epoch 475/1000\n",
      "464/464 [==============================] - 3s 8ms/step - loss: 0.5338 - accuracy: 0.7393\n",
      "Epoch 476/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5336 - accuracy: 0.7392\n",
      "Epoch 477/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5339 - accuracy: 0.7388\n",
      "Epoch 478/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5326 - accuracy: 0.7406\n",
      "Epoch 479/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5334 - accuracy: 0.7396\n",
      "Epoch 480/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5345 - accuracy: 0.7388\n",
      "Epoch 481/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5334 - accuracy: 0.7390\n",
      "Epoch 482/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5329 - accuracy: 0.7403\n",
      "Epoch 483/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5333 - accuracy: 0.7404\n",
      "Epoch 484/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5332 - accuracy: 0.7396\n",
      "Epoch 485/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5334 - accuracy: 0.7400\n",
      "Epoch 486/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5331 - accuracy: 0.7388\n",
      "Epoch 487/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5331 - accuracy: 0.7400\n",
      "Epoch 488/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5336 - accuracy: 0.7394\n",
      "Epoch 489/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5330 - accuracy: 0.7402\n",
      "Epoch 490/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5334 - accuracy: 0.7404\n",
      "Epoch 491/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5335 - accuracy: 0.7394\n",
      "Epoch 492/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5337 - accuracy: 0.7399\n",
      "Epoch 493/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5329 - accuracy: 0.7401\n",
      "Epoch 494/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5333 - accuracy: 0.7400\n",
      "Epoch 495/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5339 - accuracy: 0.7395\n",
      "Epoch 496/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5330 - accuracy: 0.7398\n",
      "Epoch 497/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5325 - accuracy: 0.7409\n",
      "Epoch 498/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5330 - accuracy: 0.7397\n",
      "Epoch 499/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5325 - accuracy: 0.7401\n",
      "Epoch 500/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5324 - accuracy: 0.7401\n",
      "Epoch 501/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5330 - accuracy: 0.7399\n",
      "Epoch 502/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5331 - accuracy: 0.7395\n",
      "Epoch 503/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5331 - accuracy: 0.7393\n",
      "Epoch 504/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5337 - accuracy: 0.7402\n",
      "Epoch 505/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5338 - accuracy: 0.7393\n",
      "Epoch 506/1000\n",
      "464/464 [==============================] - 4s 8ms/step - loss: 0.5324 - accuracy: 0.7397\n",
      "Epoch 507/1000\n",
      "464/464 [==============================] - 4s 8ms/step - loss: 0.5324 - accuracy: 0.7404\n",
      "Epoch 508/1000\n",
      "464/464 [==============================] - 6s 12ms/step - loss: 0.5327 - accuracy: 0.7399\n",
      "Epoch 509/1000\n",
      "464/464 [==============================] - 8s 18ms/step - loss: 0.5323 - accuracy: 0.7400 2s - loss: 0.534 - ETA: 2s - loss: - E\n",
      "Epoch 510/1000\n",
      "464/464 [==============================] - ETA: 0s - loss: 0.5324 - accuracy: 0.7394 ETA: 0s - loss: - 7s 15ms/step - loss: 0.5324 - accuracy: 0.7394\n",
      "Epoch 511/1000\n",
      "464/464 [==============================] - 6s 13ms/step - loss: 0.5336 - accuracy: 0.7402\n",
      "Epoch 512/1000\n",
      "464/464 [==============================] - 6s 14ms/step - loss: 0.5321 - accuracy: 0.7400 1s - loss: 0.5321 - accu - ETA: 1s - loss: 0.531 - ETA\n",
      "Epoch 513/1000\n",
      "464/464 [==============================] - 6s 12ms/step - loss: 0.5322 - accuracy: 0.7402\n",
      "Epoch 514/1000\n",
      "464/464 [==============================] - 5s 11ms/step - loss: 0.5322 - accuracy: 0.7408\n",
      "Epoch 515/1000\n",
      "464/464 [==============================] - 5s 10ms/step - loss: 0.5327 - accuracy: 0.7396\n",
      "Epoch 516/1000\n",
      "464/464 [==============================] - 5s 11ms/step - loss: 0.5334 - accuracy: 0.7390\n",
      "Epoch 517/1000\n",
      "464/464 [==============================] - 5s 10ms/step - loss: 0.5328 - accuracy: 0.7390\n",
      "Epoch 518/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5328 - accuracy: 0.7401\n",
      "Epoch 519/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5326 - accuracy: 0.7398\n",
      "Epoch 520/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5324 - accuracy: 0.7404\n",
      "Epoch 521/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5332 - accuracy: 0.7390\n",
      "Epoch 522/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5331 - accuracy: 0.7399\n",
      "Epoch 523/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5341 - accuracy: 0.7393\n",
      "Epoch 524/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5327 - accuracy: 0.7402\n",
      "Epoch 525/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5326 - accuracy: 0.7401\n",
      "Epoch 526/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5328 - accuracy: 0.7397\n",
      "Epoch 527/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5326 - accuracy: 0.7403\n",
      "Epoch 528/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5324 - accuracy: 0.7399\n",
      "Epoch 529/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5324 - accuracy: 0.7396\n",
      "Epoch 530/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5319 - accuracy: 0.7408\n",
      "Epoch 531/1000\n",
      "464/464 [==============================] - 4s 8ms/step - loss: 0.5324 - accuracy: 0.7402\n",
      "Epoch 532/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5321 - accuracy: 0.7413\n",
      "Epoch 533/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5319 - accuracy: 0.7412\n",
      "Epoch 534/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5323 - accuracy: 0.7407\n",
      "Epoch 535/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5320 - accuracy: 0.7409\n",
      "Epoch 536/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5328 - accuracy: 0.7401\n",
      "Epoch 537/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5321 - accuracy: 0.7401\n",
      "Epoch 538/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5322 - accuracy: 0.7406\n",
      "Epoch 539/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5328 - accuracy: 0.7400\n",
      "Epoch 540/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5326 - accuracy: 0.7397\n",
      "Epoch 541/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5319 - accuracy: 0.7410\n",
      "Epoch 542/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5321 - accuracy: 0.7396\n",
      "Epoch 543/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5324 - accuracy: 0.7394\n",
      "Epoch 544/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5319 - accuracy: 0.7406\n",
      "Epoch 545/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5315 - accuracy: 0.7402\n",
      "Epoch 546/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5324 - accuracy: 0.7412\n",
      "Epoch 547/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5319 - accuracy: 0.7409\n",
      "Epoch 548/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5325 - accuracy: 0.7402\n",
      "Epoch 549/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5319 - accuracy: 0.7407\n",
      "Epoch 550/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5310 - accuracy: 0.7411\n",
      "Epoch 551/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5327 - accuracy: 0.7411\n",
      "Epoch 552/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5324 - accuracy: 0.7404\n",
      "Epoch 553/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5319 - accuracy: 0.7405\n",
      "Epoch 554/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5324 - accuracy: 0.7399\n",
      "Epoch 555/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5322 - accuracy: 0.7404\n",
      "Epoch 556/1000\n",
      "464/464 [==============================] - 3s 5ms/step - loss: 0.5312 - accuracy: 0.7407\n",
      "Epoch 557/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5312 - accuracy: 0.7417\n",
      "Epoch 558/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5322 - accuracy: 0.7404\n",
      "Epoch 559/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5323 - accuracy: 0.7405\n",
      "Epoch 560/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5315 - accuracy: 0.7409\n",
      "Epoch 561/1000\n",
      "464/464 [==============================] - 3s 5ms/step - loss: 0.5319 - accuracy: 0.7414\n",
      "Epoch 562/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5314 - accuracy: 0.7411\n",
      "Epoch 563/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5315 - accuracy: 0.7404\n",
      "Epoch 564/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5316 - accuracy: 0.7417\n",
      "Epoch 565/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5320 - accuracy: 0.7416\n",
      "Epoch 566/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5315 - accuracy: 0.7409\n",
      "Epoch 567/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5323 - accuracy: 0.7409\n",
      "Epoch 568/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5313 - accuracy: 0.7408\n",
      "Epoch 569/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5312 - accuracy: 0.7411\n",
      "Epoch 570/1000\n",
      "464/464 [==============================] - 3s 5ms/step - loss: 0.5312 - accuracy: 0.7411\n",
      "Epoch 571/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5319 - accuracy: 0.7404\n",
      "Epoch 572/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5315 - accuracy: 0.7410\n",
      "Epoch 573/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5317 - accuracy: 0.7412\n",
      "Epoch 574/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5315 - accuracy: 0.7407\n",
      "Epoch 575/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5326 - accuracy: 0.7402\n",
      "Epoch 576/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5318 - accuracy: 0.7404\n",
      "Epoch 577/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5316 - accuracy: 0.7421\n",
      "Epoch 578/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5316 - accuracy: 0.7414\n",
      "Epoch 579/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5307 - accuracy: 0.7417\n",
      "Epoch 580/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5308 - accuracy: 0.7414\n",
      "Epoch 581/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5319 - accuracy: 0.7406\n",
      "Epoch 582/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5313 - accuracy: 0.7411\n",
      "Epoch 583/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5312 - accuracy: 0.7405\n",
      "Epoch 584/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5319 - accuracy: 0.7412\n",
      "Epoch 585/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5315 - accuracy: 0.7410\n",
      "Epoch 586/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5313 - accuracy: 0.7413\n",
      "Epoch 587/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5312 - accuracy: 0.7412\n",
      "Epoch 588/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5313 - accuracy: 0.7412\n",
      "Epoch 589/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5308 - accuracy: 0.7408\n",
      "Epoch 590/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5308 - accuracy: 0.7416\n",
      "Epoch 591/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5306 - accuracy: 0.7421\n",
      "Epoch 592/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5308 - accuracy: 0.7414\n",
      "Epoch 593/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5311 - accuracy: 0.7422\n",
      "Epoch 594/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5307 - accuracy: 0.7417\n",
      "Epoch 595/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5310 - accuracy: 0.7416\n",
      "Epoch 596/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5306 - accuracy: 0.7409\n",
      "Epoch 597/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5309 - accuracy: 0.7411\n",
      "Epoch 598/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5316 - accuracy: 0.7407\n",
      "Epoch 599/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5313 - accuracy: 0.7408\n",
      "Epoch 600/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5305 - accuracy: 0.7413\n",
      "Epoch 601/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5311 - accuracy: 0.7402\n",
      "Epoch 602/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5314 - accuracy: 0.7411\n",
      "Epoch 603/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5314 - accuracy: 0.7394\n",
      "Epoch 604/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5323 - accuracy: 0.7405\n",
      "Epoch 605/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5314 - accuracy: 0.7403\n",
      "Epoch 606/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5310 - accuracy: 0.7418\n",
      "Epoch 607/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5311 - accuracy: 0.7411\n",
      "Epoch 608/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5319 - accuracy: 0.7399\n",
      "Epoch 609/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5306 - accuracy: 0.7418\n",
      "Epoch 610/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5311 - accuracy: 0.7410\n",
      "Epoch 611/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5311 - accuracy: 0.7417\n",
      "Epoch 612/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5302 - accuracy: 0.7418\n",
      "Epoch 613/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5306 - accuracy: 0.7409\n",
      "Epoch 614/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5303 - accuracy: 0.7417\n",
      "Epoch 615/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5309 - accuracy: 0.7414\n",
      "Epoch 616/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5308 - accuracy: 0.7414\n",
      "Epoch 617/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5308 - accuracy: 0.7415\n",
      "Epoch 618/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5313 - accuracy: 0.7420\n",
      "Epoch 619/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5298 - accuracy: 0.7413\n",
      "Epoch 620/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5309 - accuracy: 0.7412\n",
      "Epoch 621/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5302 - accuracy: 0.7422\n",
      "Epoch 622/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5299 - accuracy: 0.7411\n",
      "Epoch 623/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5320 - accuracy: 0.7413\n",
      "Epoch 624/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5300 - accuracy: 0.7420\n",
      "Epoch 625/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5300 - accuracy: 0.7420\n",
      "Epoch 626/1000\n",
      "464/464 [==============================] - ETA: 0s - loss: 0.5303 - accuracy: 0.74 - 2s 4ms/step - loss: 0.5304 - accuracy: 0.7419\n",
      "Epoch 627/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5306 - accuracy: 0.7421\n",
      "Epoch 628/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5304 - accuracy: 0.7417\n",
      "Epoch 629/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5303 - accuracy: 0.7415\n",
      "Epoch 630/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5304 - accuracy: 0.7415\n",
      "Epoch 631/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5311 - accuracy: 0.7411\n",
      "Epoch 632/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5298 - accuracy: 0.7418\n",
      "Epoch 633/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5304 - accuracy: 0.7416\n",
      "Epoch 634/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5315 - accuracy: 0.7407\n",
      "Epoch 635/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5303 - accuracy: 0.7411\n",
      "Epoch 636/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5309 - accuracy: 0.7421\n",
      "Epoch 637/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5308 - accuracy: 0.7414\n",
      "Epoch 638/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5302 - accuracy: 0.7421\n",
      "Epoch 639/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5295 - accuracy: 0.7421\n",
      "Epoch 640/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5306 - accuracy: 0.7419\n",
      "Epoch 641/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5302 - accuracy: 0.7422\n",
      "Epoch 642/1000\n",
      "464/464 [==============================] - 3s 5ms/step - loss: 0.5301 - accuracy: 0.7424\n",
      "Epoch 643/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5313 - accuracy: 0.7416\n",
      "Epoch 644/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5312 - accuracy: 0.7417\n",
      "Epoch 645/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5302 - accuracy: 0.7423\n",
      "Epoch 646/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5302 - accuracy: 0.7416\n",
      "Epoch 647/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5301 - accuracy: 0.7417\n",
      "Epoch 648/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5306 - accuracy: 0.7416\n",
      "Epoch 649/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5307 - accuracy: 0.7415\n",
      "Epoch 650/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5297 - accuracy: 0.7427\n",
      "Epoch 651/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5301 - accuracy: 0.7416\n",
      "Epoch 652/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5304 - accuracy: 0.7417\n",
      "Epoch 653/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5313 - accuracy: 0.7412\n",
      "Epoch 654/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5291 - accuracy: 0.7426\n",
      "Epoch 655/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5297 - accuracy: 0.7425\n",
      "Epoch 656/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5299 - accuracy: 0.7426\n",
      "Epoch 657/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5302 - accuracy: 0.7422\n",
      "Epoch 658/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5306 - accuracy: 0.7418\n",
      "Epoch 659/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5302 - accuracy: 0.7424\n",
      "Epoch 660/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5301 - accuracy: 0.7424\n",
      "Epoch 661/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5297 - accuracy: 0.7431\n",
      "Epoch 662/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5301 - accuracy: 0.7420\n",
      "Epoch 663/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5296 - accuracy: 0.7422\n",
      "Epoch 664/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5301 - accuracy: 0.7414\n",
      "Epoch 665/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5300 - accuracy: 0.7427\n",
      "Epoch 666/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5298 - accuracy: 0.7412\n",
      "Epoch 667/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5297 - accuracy: 0.7424\n",
      "Epoch 668/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5295 - accuracy: 0.7423\n",
      "Epoch 669/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5292 - accuracy: 0.7426\n",
      "Epoch 670/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5296 - accuracy: 0.7426\n",
      "Epoch 671/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5295 - accuracy: 0.7430\n",
      "Epoch 672/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5302 - accuracy: 0.7417\n",
      "Epoch 673/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5306 - accuracy: 0.7413\n",
      "Epoch 674/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5299 - accuracy: 0.7427\n",
      "Epoch 675/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5293 - accuracy: 0.7425\n",
      "Epoch 676/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5303 - accuracy: 0.7421\n",
      "Epoch 677/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5304 - accuracy: 0.7411\n",
      "Epoch 678/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5299 - accuracy: 0.7424\n",
      "Epoch 679/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5296 - accuracy: 0.7425\n",
      "Epoch 680/1000\n",
      "464/464 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.74 - 2s 4ms/step - loss: 0.5299 - accuracy: 0.7419\n",
      "Epoch 681/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5291 - accuracy: 0.7432\n",
      "Epoch 682/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5291 - accuracy: 0.7424\n",
      "Epoch 683/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5292 - accuracy: 0.7426\n",
      "Epoch 684/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5305 - accuracy: 0.7414\n",
      "Epoch 685/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5294 - accuracy: 0.7426\n",
      "Epoch 686/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5292 - accuracy: 0.7433\n",
      "Epoch 687/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5301 - accuracy: 0.7420\n",
      "Epoch 688/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5293 - accuracy: 0.7421\n",
      "Epoch 689/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5294 - accuracy: 0.7422\n",
      "Epoch 690/1000\n",
      "464/464 [==============================] - 3s 5ms/step - loss: 0.5290 - accuracy: 0.7437\n",
      "Epoch 691/1000\n",
      "464/464 [==============================] - 3s 5ms/step - loss: 0.5296 - accuracy: 0.7426\n",
      "Epoch 692/1000\n",
      "464/464 [==============================] - 3s 5ms/step - loss: 0.5309 - accuracy: 0.7416\n",
      "Epoch 693/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5291 - accuracy: 0.7415\n",
      "Epoch 694/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5300 - accuracy: 0.7413\n",
      "Epoch 695/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5290 - accuracy: 0.7423\n",
      "Epoch 696/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5300 - accuracy: 0.7423\n",
      "Epoch 697/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5293 - accuracy: 0.7422\n",
      "Epoch 698/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5302 - accuracy: 0.7423\n",
      "Epoch 699/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5295 - accuracy: 0.7420\n",
      "Epoch 700/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5292 - accuracy: 0.7422\n",
      "Epoch 701/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5301 - accuracy: 0.7417\n",
      "Epoch 702/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5296 - accuracy: 0.7422\n",
      "Epoch 703/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5293 - accuracy: 0.7417\n",
      "Epoch 704/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5297 - accuracy: 0.7420\n",
      "Epoch 705/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5297 - accuracy: 0.7422\n",
      "Epoch 706/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5298 - accuracy: 0.7418\n",
      "Epoch 707/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5292 - accuracy: 0.7421\n",
      "Epoch 708/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5304 - accuracy: 0.7416\n",
      "Epoch 709/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5301 - accuracy: 0.7421\n",
      "Epoch 710/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5300 - accuracy: 0.7431\n",
      "Epoch 711/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5291 - accuracy: 0.7429: 0s - loss: 0.5296 - ac\n",
      "Epoch 712/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5291 - accuracy: 0.7425\n",
      "Epoch 713/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5291 - accuracy: 0.7429\n",
      "Epoch 714/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5297 - accuracy: 0.7424\n",
      "Epoch 715/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5288 - accuracy: 0.7434\n",
      "Epoch 716/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5290 - accuracy: 0.7430\n",
      "Epoch 717/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5298 - accuracy: 0.7430\n",
      "Epoch 718/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5287 - accuracy: 0.7430\n",
      "Epoch 719/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5289 - accuracy: 0.7429\n",
      "Epoch 720/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5293 - accuracy: 0.7430\n",
      "Epoch 721/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5297 - accuracy: 0.7423\n",
      "Epoch 722/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5297 - accuracy: 0.7423\n",
      "Epoch 723/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5296 - accuracy: 0.7413\n",
      "Epoch 724/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5295 - accuracy: 0.7421\n",
      "Epoch 725/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5298 - accuracy: 0.7426\n",
      "Epoch 726/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5295 - accuracy: 0.7425\n",
      "Epoch 727/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5293 - accuracy: 0.7422\n",
      "Epoch 728/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5293 - accuracy: 0.7426\n",
      "Epoch 729/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5288 - accuracy: 0.7424\n",
      "Epoch 730/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5300 - accuracy: 0.7428\n",
      "Epoch 731/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5296 - accuracy: 0.7421\n",
      "Epoch 732/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5299 - accuracy: 0.7427\n",
      "Epoch 733/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5288 - accuracy: 0.7432\n",
      "Epoch 734/1000\n",
      "464/464 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.74 - 2s 5ms/step - loss: 0.5292 - accuracy: 0.7427\n",
      "Epoch 735/1000\n",
      "464/464 [==============================] - ETA: 0s - loss: 0.5295 - accuracy: 0.74 - 2s 5ms/step - loss: 0.5296 - accuracy: 0.7416\n",
      "Epoch 736/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5294 - accuracy: 0.7429\n",
      "Epoch 737/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5288 - accuracy: 0.7428\n",
      "Epoch 738/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5294 - accuracy: 0.7430\n",
      "Epoch 739/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5287 - accuracy: 0.7431\n",
      "Epoch 740/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5292 - accuracy: 0.7430\n",
      "Epoch 741/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5293 - accuracy: 0.7426\n",
      "Epoch 742/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5300 - accuracy: 0.7418\n",
      "Epoch 743/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5295 - accuracy: 0.7424\n",
      "Epoch 744/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5298 - accuracy: 0.7427\n",
      "Epoch 745/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5296 - accuracy: 0.7426\n",
      "Epoch 746/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5299 - accuracy: 0.7424\n",
      "Epoch 747/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5295 - accuracy: 0.7425\n",
      "Epoch 748/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5293 - accuracy: 0.7427\n",
      "Epoch 749/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5286 - accuracy: 0.7435\n",
      "Epoch 750/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5292 - accuracy: 0.7430\n",
      "Epoch 751/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5295 - accuracy: 0.7421\n",
      "Epoch 752/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5288 - accuracy: 0.7426\n",
      "Epoch 753/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5297 - accuracy: 0.7426\n",
      "Epoch 754/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5293 - accuracy: 0.7418\n",
      "Epoch 755/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5289 - accuracy: 0.7426\n",
      "Epoch 756/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5286 - accuracy: 0.7435\n",
      "Epoch 757/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5288 - accuracy: 0.7432\n",
      "Epoch 758/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5297 - accuracy: 0.7419\n",
      "Epoch 759/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5300 - accuracy: 0.7423\n",
      "Epoch 760/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5294 - accuracy: 0.7417\n",
      "Epoch 761/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5300 - accuracy: 0.7429\n",
      "Epoch 762/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5289 - accuracy: 0.7427\n",
      "Epoch 763/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5298 - accuracy: 0.7425\n",
      "Epoch 764/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5286 - accuracy: 0.7425\n",
      "Epoch 765/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5287 - accuracy: 0.7431\n",
      "Epoch 766/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5300 - accuracy: 0.7427\n",
      "Epoch 767/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5291 - accuracy: 0.7427\n",
      "Epoch 768/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5290 - accuracy: 0.7436\n",
      "Epoch 769/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5287 - accuracy: 0.7432\n",
      "Epoch 770/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5292 - accuracy: 0.7432\n",
      "Epoch 771/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5292 - accuracy: 0.7431\n",
      "Epoch 772/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5296 - accuracy: 0.7432\n",
      "Epoch 773/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5283 - accuracy: 0.7432\n",
      "Epoch 774/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5295 - accuracy: 0.7430\n",
      "Epoch 775/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5296 - accuracy: 0.7424\n",
      "Epoch 776/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5283 - accuracy: 0.7425\n",
      "Epoch 777/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5287 - accuracy: 0.7425\n",
      "Epoch 778/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5282 - accuracy: 0.7434\n",
      "Epoch 779/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5296 - accuracy: 0.7419\n",
      "Epoch 780/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5292 - accuracy: 0.7430\n",
      "Epoch 781/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5289 - accuracy: 0.7430\n",
      "Epoch 782/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5283 - accuracy: 0.7440\n",
      "Epoch 783/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5285 - accuracy: 0.7429\n",
      "Epoch 784/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5283 - accuracy: 0.7435\n",
      "Epoch 785/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5281 - accuracy: 0.7439\n",
      "Epoch 786/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5285 - accuracy: 0.7428\n",
      "Epoch 787/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5287 - accuracy: 0.7424\n",
      "Epoch 788/1000\n",
      "464/464 [==============================] - 3s 5ms/step - loss: 0.5285 - accuracy: 0.7428\n",
      "Epoch 789/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5289 - accuracy: 0.7425\n",
      "Epoch 790/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5284 - accuracy: 0.7442\n",
      "Epoch 791/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5288 - accuracy: 0.7429\n",
      "Epoch 792/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5286 - accuracy: 0.7429\n",
      "Epoch 793/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5281 - accuracy: 0.7445\n",
      "Epoch 794/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5288 - accuracy: 0.7431\n",
      "Epoch 795/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5284 - accuracy: 0.7433\n",
      "Epoch 796/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5278 - accuracy: 0.7433\n",
      "Epoch 797/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5281 - accuracy: 0.7439\n",
      "Epoch 798/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5286 - accuracy: 0.7428\n",
      "Epoch 799/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5284 - accuracy: 0.7424\n",
      "Epoch 800/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5288 - accuracy: 0.7427\n",
      "Epoch 801/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5287 - accuracy: 0.7430\n",
      "Epoch 802/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5288 - accuracy: 0.7434\n",
      "Epoch 803/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5277 - accuracy: 0.7437\n",
      "Epoch 804/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5284 - accuracy: 0.7430\n",
      "Epoch 805/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5282 - accuracy: 0.7439\n",
      "Epoch 806/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5282 - accuracy: 0.7431\n",
      "Epoch 807/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5293 - accuracy: 0.7425\n",
      "Epoch 808/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5284 - accuracy: 0.7428\n",
      "Epoch 809/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5280 - accuracy: 0.7436\n",
      "Epoch 810/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7439\n",
      "Epoch 811/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5273 - accuracy: 0.7441\n",
      "Epoch 812/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7435\n",
      "Epoch 813/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5284 - accuracy: 0.7440\n",
      "Epoch 814/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5292 - accuracy: 0.7428\n",
      "Epoch 815/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5290 - accuracy: 0.7436\n",
      "Epoch 816/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5289 - accuracy: 0.7435\n",
      "Epoch 817/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5282 - accuracy: 0.7435\n",
      "Epoch 818/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5281 - accuracy: 0.7441\n",
      "Epoch 819/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5291 - accuracy: 0.7437\n",
      "Epoch 820/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5285 - accuracy: 0.7440\n",
      "Epoch 821/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5282 - accuracy: 0.7448\n",
      "Epoch 822/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5282 - accuracy: 0.7441\n",
      "Epoch 823/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5283 - accuracy: 0.7437\n",
      "Epoch 824/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5285 - accuracy: 0.7442\n",
      "Epoch 825/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5283 - accuracy: 0.7426\n",
      "Epoch 826/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5277 - accuracy: 0.7436\n",
      "Epoch 827/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5284 - accuracy: 0.7426\n",
      "Epoch 828/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5277 - accuracy: 0.7442\n",
      "Epoch 829/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5284 - accuracy: 0.7435\n",
      "Epoch 830/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7437\n",
      "Epoch 831/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5274 - accuracy: 0.7435\n",
      "Epoch 832/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5292 - accuracy: 0.7428\n",
      "Epoch 833/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5287 - accuracy: 0.7424\n",
      "Epoch 834/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5281 - accuracy: 0.7430\n",
      "Epoch 835/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5280 - accuracy: 0.7432\n",
      "Epoch 836/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5277 - accuracy: 0.7441\n",
      "Epoch 837/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5286 - accuracy: 0.7430\n",
      "Epoch 838/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5279 - accuracy: 0.7432\n",
      "Epoch 839/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5282 - accuracy: 0.7429\n",
      "Epoch 840/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5291 - accuracy: 0.7429\n",
      "Epoch 841/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7432\n",
      "Epoch 842/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5277 - accuracy: 0.7439\n",
      "Epoch 843/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5279 - accuracy: 0.7430\n",
      "Epoch 844/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5282 - accuracy: 0.7435\n",
      "Epoch 845/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5285 - accuracy: 0.7439\n",
      "Epoch 846/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5281 - accuracy: 0.7437\n",
      "Epoch 847/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5276 - accuracy: 0.7437\n",
      "Epoch 848/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5283 - accuracy: 0.7432\n",
      "Epoch 849/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5282 - accuracy: 0.7435\n",
      "Epoch 850/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5284 - accuracy: 0.7436\n",
      "Epoch 851/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5278 - accuracy: 0.7442\n",
      "Epoch 852/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5275 - accuracy: 0.7442\n",
      "Epoch 853/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5283 - accuracy: 0.7428\n",
      "Epoch 854/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5282 - accuracy: 0.7437\n",
      "Epoch 855/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5285 - accuracy: 0.7434\n",
      "Epoch 856/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5273 - accuracy: 0.7443\n",
      "Epoch 857/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5275 - accuracy: 0.7442\n",
      "Epoch 858/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7442\n",
      "Epoch 859/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5279 - accuracy: 0.7450\n",
      "Epoch 860/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5279 - accuracy: 0.7434\n",
      "Epoch 861/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5283 - accuracy: 0.7427\n",
      "Epoch 862/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5277 - accuracy: 0.7440\n",
      "Epoch 863/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5281 - accuracy: 0.7441\n",
      "Epoch 864/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5289 - accuracy: 0.7420\n",
      "Epoch 865/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5286 - accuracy: 0.7439\n",
      "Epoch 866/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5284 - accuracy: 0.7431\n",
      "Epoch 867/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5288 - accuracy: 0.7431\n",
      "Epoch 868/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5281 - accuracy: 0.7428\n",
      "Epoch 869/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5288 - accuracy: 0.7422\n",
      "Epoch 870/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5286 - accuracy: 0.7429\n",
      "Epoch 871/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5285 - accuracy: 0.7425\n",
      "Epoch 872/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5290 - accuracy: 0.7423\n",
      "Epoch 873/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7428\n",
      "Epoch 874/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5276 - accuracy: 0.7437\n",
      "Epoch 875/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5275 - accuracy: 0.7439\n",
      "Epoch 876/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5279 - accuracy: 0.7440\n",
      "Epoch 877/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5281 - accuracy: 0.7428\n",
      "Epoch 878/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5290 - accuracy: 0.7427\n",
      "Epoch 879/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5282 - accuracy: 0.7430\n",
      "Epoch 880/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5274 - accuracy: 0.7441\n",
      "Epoch 881/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5286 - accuracy: 0.7442\n",
      "Epoch 882/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5282 - accuracy: 0.7434\n",
      "Epoch 883/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7448\n",
      "Epoch 884/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5280 - accuracy: 0.7431\n",
      "Epoch 885/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7442\n",
      "Epoch 886/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7437\n",
      "Epoch 887/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7434\n",
      "Epoch 888/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5279 - accuracy: 0.7437\n",
      "Epoch 889/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5282 - accuracy: 0.7432\n",
      "Epoch 890/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5280 - accuracy: 0.7439\n",
      "Epoch 891/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5285 - accuracy: 0.7432\n",
      "Epoch 892/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5281 - accuracy: 0.7437\n",
      "Epoch 893/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5286 - accuracy: 0.7430\n",
      "Epoch 894/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5275 - accuracy: 0.7434\n",
      "Epoch 895/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5277 - accuracy: 0.7440\n",
      "Epoch 896/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5276 - accuracy: 0.7439\n",
      "Epoch 897/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5278 - accuracy: 0.7443\n",
      "Epoch 898/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5278 - accuracy: 0.7441\n",
      "Epoch 899/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5272 - accuracy: 0.7442\n",
      "Epoch 900/1000\n",
      "464/464 [==============================] - 5s 10ms/step - loss: 0.5276 - accuracy: 0.7439\n",
      "Epoch 901/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5272 - accuracy: 0.7443\n",
      "Epoch 902/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5279 - accuracy: 0.7442\n",
      "Epoch 903/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5276 - accuracy: 0.7445\n",
      "Epoch 904/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5276 - accuracy: 0.7447\n",
      "Epoch 905/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5282 - accuracy: 0.7439\n",
      "Epoch 906/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5275 - accuracy: 0.7441\n",
      "Epoch 907/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5279 - accuracy: 0.7444\n",
      "Epoch 908/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5265 - accuracy: 0.7449\n",
      "Epoch 909/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5268 - accuracy: 0.7447\n",
      "Epoch 910/1000\n",
      "464/464 [==============================] - 5s 11ms/step - loss: 0.5280 - accuracy: 0.7443\n",
      "Epoch 911/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5277 - accuracy: 0.7428\n",
      "Epoch 912/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5273 - accuracy: 0.7444\n",
      "Epoch 913/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5272 - accuracy: 0.7443\n",
      "Epoch 914/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5269 - accuracy: 0.7449\n",
      "Epoch 915/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5276 - accuracy: 0.7437\n",
      "Epoch 916/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5276 - accuracy: 0.7444\n",
      "Epoch 917/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5268 - accuracy: 0.7441\n",
      "Epoch 918/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5272 - accuracy: 0.7440\n",
      "Epoch 919/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5268 - accuracy: 0.7437\n",
      "Epoch 920/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5275 - accuracy: 0.7434\n",
      "Epoch 921/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5278 - accuracy: 0.7438\n",
      "Epoch 922/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5274 - accuracy: 0.7447\n",
      "Epoch 923/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7438\n",
      "Epoch 924/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5276 - accuracy: 0.7441\n",
      "Epoch 925/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5278 - accuracy: 0.7438\n",
      "Epoch 926/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5277 - accuracy: 0.7442\n",
      "Epoch 927/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5281 - accuracy: 0.7441\n",
      "Epoch 928/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5275 - accuracy: 0.7442\n",
      "Epoch 929/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5277 - accuracy: 0.7437\n",
      "Epoch 930/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7437\n",
      "Epoch 931/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5281 - accuracy: 0.7434\n",
      "Epoch 932/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5276 - accuracy: 0.7436\n",
      "Epoch 933/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5273 - accuracy: 0.7439\n",
      "Epoch 934/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5271 - accuracy: 0.7444\n",
      "Epoch 935/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5271 - accuracy: 0.7444\n",
      "Epoch 936/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7439\n",
      "Epoch 937/1000\n",
      "464/464 [==============================] - ETA: 0s - loss: 0.5270 - accuracy: 0.74 - 2s 4ms/step - loss: 0.5270 - accuracy: 0.7446\n",
      "Epoch 938/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5280 - accuracy: 0.7441\n",
      "Epoch 939/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5270 - accuracy: 0.7444\n",
      "Epoch 940/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5273 - accuracy: 0.7443\n",
      "Epoch 941/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5275 - accuracy: 0.7440\n",
      "Epoch 942/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5270 - accuracy: 0.7445\n",
      "Epoch 943/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5264 - accuracy: 0.7452\n",
      "Epoch 944/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5271 - accuracy: 0.7438\n",
      "Epoch 945/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5266 - accuracy: 0.7444\n",
      "Epoch 946/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5272 - accuracy: 0.7442\n",
      "Epoch 947/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5269 - accuracy: 0.7444\n",
      "Epoch 948/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5272 - accuracy: 0.7442\n",
      "Epoch 949/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5268 - accuracy: 0.7440\n",
      "Epoch 950/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5270 - accuracy: 0.7449\n",
      "Epoch 951/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5267 - accuracy: 0.7448\n",
      "Epoch 952/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5267 - accuracy: 0.7444\n",
      "Epoch 953/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5273 - accuracy: 0.7438\n",
      "Epoch 954/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5273 - accuracy: 0.7441\n",
      "Epoch 955/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5273 - accuracy: 0.7439\n",
      "Epoch 956/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5269 - accuracy: 0.7430\n",
      "Epoch 957/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5273 - accuracy: 0.7439\n",
      "Epoch 958/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5284 - accuracy: 0.7430\n",
      "Epoch 959/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5278 - accuracy: 0.7436\n",
      "Epoch 960/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5277 - accuracy: 0.7438\n",
      "Epoch 961/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5270 - accuracy: 0.7445\n",
      "Epoch 962/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5272 - accuracy: 0.7435\n",
      "Epoch 963/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5276 - accuracy: 0.7439\n",
      "Epoch 964/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5274 - accuracy: 0.7443\n",
      "Epoch 965/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5279 - accuracy: 0.7433\n",
      "Epoch 966/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5276 - accuracy: 0.7443\n",
      "Epoch 967/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5281 - accuracy: 0.7434\n",
      "Epoch 968/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5271 - accuracy: 0.7439\n",
      "Epoch 969/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7441\n",
      "Epoch 970/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5281 - accuracy: 0.7440\n",
      "Epoch 971/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5276 - accuracy: 0.7437\n",
      "Epoch 972/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5274 - accuracy: 0.7453\n",
      "Epoch 973/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5274 - accuracy: 0.7441\n",
      "Epoch 974/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5274 - accuracy: 0.7435\n",
      "Epoch 975/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5272 - accuracy: 0.7441\n",
      "Epoch 976/1000\n",
      "464/464 [==============================] - 3s 6ms/step - loss: 0.5275 - accuracy: 0.7443\n",
      "Epoch 977/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5263 - accuracy: 0.7450\n",
      "Epoch 978/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5271 - accuracy: 0.7448\n",
      "Epoch 979/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5277 - accuracy: 0.7445\n",
      "Epoch 980/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5272 - accuracy: 0.7445\n",
      "Epoch 981/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5268 - accuracy: 0.7445\n",
      "Epoch 982/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5269 - accuracy: 0.7451\n",
      "Epoch 983/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5266 - accuracy: 0.7445\n",
      "Epoch 984/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5276 - accuracy: 0.7440\n",
      "Epoch 985/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5276 - accuracy: 0.7439\n",
      "Epoch 986/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5278 - accuracy: 0.7433\n",
      "Epoch 987/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5274 - accuracy: 0.7441\n",
      "Epoch 988/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5267 - accuracy: 0.7447\n",
      "Epoch 989/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5270 - accuracy: 0.7443\n",
      "Epoch 990/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5273 - accuracy: 0.7445\n",
      "Epoch 991/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5269 - accuracy: 0.7447\n",
      "Epoch 992/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5261 - accuracy: 0.7454\n",
      "Epoch 993/1000\n",
      "464/464 [==============================] - 2s 5ms/step - loss: 0.5271 - accuracy: 0.7442\n",
      "Epoch 994/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5267 - accuracy: 0.7446\n",
      "Epoch 995/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5269 - accuracy: 0.7442\n",
      "Epoch 996/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5266 - accuracy: 0.7456\n",
      "Epoch 997/1000\n",
      "464/464 [==============================] - 3s 5ms/step - loss: 0.5271 - accuracy: 0.7450\n",
      "Epoch 998/1000\n",
      "464/464 [==============================] - 3s 7ms/step - loss: 0.5268 - accuracy: 0.7449\n",
      "Epoch 999/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5269 - accuracy: 0.7451\n",
      "Epoch 1000/1000\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.5266 - accuracy: 0.7447\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "#--------------First Hidden Layer----------------------------#\n",
    "cnn.add(Dense(units = 8, activation='relu', kernel_initializer='he_uniform',input_dim= len(X_train.columns)))\n",
    "\n",
    "#--------------Second  Hidden Layer -------------------------#\n",
    "cnn.add(Dense(4, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "#--------------Third Hidden Layer -------------------------#\n",
    "cnn.add(Dense(2, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "#--------------Output Layer----------------------------------#\n",
    "cnn.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "\n",
    "#--------------Compiling the neural network------------------#\n",
    "cnn.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "\n",
    "model = cnn.fit(X_train,y_train, batch_size=100, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.703418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.703418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.703418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.322655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19844</th>\n",
       "      <td>0.703418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19845</th>\n",
       "      <td>0.703418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19846</th>\n",
       "      <td>0.380799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19847</th>\n",
       "      <td>0.703418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19848</th>\n",
       "      <td>0.278992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19849 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0      0.703418\n",
       "1      0.703418\n",
       "2      0.703418\n",
       "3      0.322655\n",
       "4      0.007281\n",
       "...         ...\n",
       "19844  0.703418\n",
       "19845  0.703418\n",
       "19846  0.380799\n",
       "19847  0.703418\n",
       "19848  0.278992\n",
       "\n",
       "[19849 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_predict = pd.DataFrame(cnn.predict(X_test))\n",
    "cnn_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prob_1</th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.703418</td>\n",
       "      <td>0</td>\n",
       "      <td>23110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.703418</td>\n",
       "      <td>1</td>\n",
       "      <td>44220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.703418</td>\n",
       "      <td>2</td>\n",
       "      <td>30942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.322655</td>\n",
       "      <td>3</td>\n",
       "      <td>33417</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007281</td>\n",
       "      <td>4</td>\n",
       "      <td>43562</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19844</th>\n",
       "      <td>0.703418</td>\n",
       "      <td>19844</td>\n",
       "      <td>61162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19845</th>\n",
       "      <td>0.703418</td>\n",
       "      <td>19845</td>\n",
       "      <td>41122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19846</th>\n",
       "      <td>0.380799</td>\n",
       "      <td>19846</td>\n",
       "      <td>16689</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19847</th>\n",
       "      <td>0.703418</td>\n",
       "      <td>19847</td>\n",
       "      <td>18778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19848</th>\n",
       "      <td>0.278992</td>\n",
       "      <td>19848</td>\n",
       "      <td>54373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19849 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Prob_1  level_0  index  return\n",
       "0      0.703418        0  23110       1\n",
       "1      0.703418        1  44220       1\n",
       "2      0.703418        2  30942       1\n",
       "3      0.322655        3  33417       0\n",
       "4      0.007281        4  43562       1\n",
       "...         ...      ...    ...     ...\n",
       "19844  0.703418    19844  61162       0\n",
       "19845  0.703418    19845  41122       1\n",
       "19846  0.380799    19846  16689       1\n",
       "19847  0.703418    19847  18778       0\n",
       "19848  0.278992    19848  54373       0\n",
       "\n",
       "[19849 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred_cnn = pd.concat([cnn_predict,y_test],axis =1)\n",
    "\n",
    "final_pred_cnn.columns =['Prob_1','level_0','index','return']\n",
    "final_pred_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the predcited column in the data using probablity cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted_mean</th>\n",
       "      <th>Predicted_1</th>\n",
       "      <th>Predicted_all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.550241</td>\n",
       "      <td>5476</td>\n",
       "      <td>9952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.690714</td>\n",
       "      <td>6836</td>\n",
       "      <td>9897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Predicted_mean  Predicted_1  Predicted_all\n",
       "return                                            \n",
       "0             0.550241         5476           9952\n",
       "1             0.690714         6836           9897"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_pred.columns =['Prob_1','index','return']\n",
    "final_pred_cnn['Predicted_cnn'] = np.where(final_pred_cnn['Prob_1']>=0.65,1,0)\n",
    "final_pred_cnn.groupby(['return']).agg(Predicted_mean=('Predicted_cnn', 'mean'),Predicted_1=('Predicted_cnn', 'sum'),Predicted_all=('Predicted_cnn', 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5699027658824122"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy_table =final_pred_cnn.groupby(['return']).agg(Predicted_mean=('Predicted_cnn', 'mean'),Predicted_1=('Predicted_cnn', 'sum'),Predicted_0=('Predicted_cnn', 'count'))\n",
    "Accuracy_table= Accuracy_table.reset_index()\n",
    "Accuracy_table.columns = ['Actual_return','predicted_mean','predicted_1','predicted_all']\n",
    "Accuracy_table['predicted_0'] = Accuracy_table['predicted_all'] - Accuracy_table['predicted_1']\n",
    "Accuracy = (Accuracy_table['predicted_0'][0] + Accuracy_table['predicted_1'][1] )/Accuracy_table['predicted_all'].sum()\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prob_1</th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>return</th>\n",
       "      <th>Predicted_cnn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.703418</td>\n",
       "      <td>0</td>\n",
       "      <td>23110</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.703418</td>\n",
       "      <td>1</td>\n",
       "      <td>44220</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.703418</td>\n",
       "      <td>2</td>\n",
       "      <td>30942</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.322655</td>\n",
       "      <td>3</td>\n",
       "      <td>33417</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007281</td>\n",
       "      <td>4</td>\n",
       "      <td>43562</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19844</th>\n",
       "      <td>0.703418</td>\n",
       "      <td>19844</td>\n",
       "      <td>61162</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19845</th>\n",
       "      <td>0.703418</td>\n",
       "      <td>19845</td>\n",
       "      <td>41122</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19846</th>\n",
       "      <td>0.380799</td>\n",
       "      <td>19846</td>\n",
       "      <td>16689</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19847</th>\n",
       "      <td>0.703418</td>\n",
       "      <td>19847</td>\n",
       "      <td>18778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19848</th>\n",
       "      <td>0.278992</td>\n",
       "      <td>19848</td>\n",
       "      <td>54373</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19849 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Prob_1  level_0  index  return  Predicted_cnn\n",
       "0      0.703418        0  23110       1              1\n",
       "1      0.703418        1  44220       1              1\n",
       "2      0.703418        2  30942       1              1\n",
       "3      0.322655        3  33417       0              0\n",
       "4      0.007281        4  43562       1              0\n",
       "...         ...      ...    ...     ...            ...\n",
       "19844  0.703418    19844  61162       0              1\n",
       "19845  0.703418    19845  41122       1              1\n",
       "19846  0.380799    19846  16689       1              0\n",
       "19847  0.703418    19847  18778       0              1\n",
       "19848  0.278992    19848  54373       0              0\n",
       "\n",
       "[19849 rows x 5 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the confusion matrix table to asses the model perfromance and create lift chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p_table =pd.DataFrame()\n",
    "\n",
    "list_a = [10,20,30,40,50,60,70,80,90]\n",
    "\n",
    "i=0\n",
    "\n",
    "for n in list_a:\n",
    "           \n",
    "\n",
    "            p_table.loc[i,'Percentile'] = n\n",
    "\n",
    "            p_table.loc[i,'Cut_Off_Probability'] = np.percentile(np.array(final_pred_cnn['Prob_1']),n)\n",
    "\n",
    "            cut_off = p_table.loc[i,'Cut_Off_Probability']\n",
    "\n",
    "            p_table.loc[i,'TN'] = len(final_pred_cnn[final_pred_cnn['return'] == 0]\\\n",
    "                                      [final_pred_cnn['Prob_1'] < cut_off])\n",
    "\n",
    "            p_table.loc[i,'FP'] = len(final_pred_cnn[final_pred_cnn['return'] == 0]\\\n",
    "                                      [final_pred_cnn['Prob_1'] >= cut_off])\n",
    "\n",
    "            p_table.loc[i,'FN'] = len(final_pred_cnn[final_pred_cnn['return'] == 1]\\\n",
    "                                      [final_pred_cnn['Prob_1'] < cut_off])\n",
    "\n",
    "            p_table.loc[i,'TP'] = len(final_pred_cnn[final_pred_cnn['return'] == 1]\\\n",
    "                                      [final_pred_cnn['Prob_1'] >= cut_off])\n",
    "\n",
    "            p_table.loc[i,'Total'] = p_table.loc[i,'TN'] + p_table.loc[i,'FP'] + p_table.loc[i,'FN'] + p_table.loc[i,'TP']\n",
    "\n",
    "            p_table.loc[i,'FP+TP'] = p_table.loc[i,'FP'] + p_table.loc[i,'TP']\n",
    "\n",
    "        \n",
    "\n",
    "            p_table.loc[i,'Predicted 1 Accuracy'] = p_table.loc[i,'TP']/(p_table.loc[i,'TP'] + p_table.loc[i,'FP'])\n",
    "\n",
    "          \n",
    "\n",
    "            i+=1\n",
    "\n",
    "\n",
    "\n",
    "p_table.to_csv(\"Cnn_lift_chart.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN Models are not perfroming and will need to be iterated further for improvments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4  - Prediction of the Validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create some derived varibale in the data \n",
    "\n",
    "df_validation = pd.read_csv(path + \"TestingData_For_Candidate.csv\")\n",
    "\n",
    "df_validation['delivery_date']=pd.to_datetime(df_validation['delivery_date'],format = '%m/%d/%Y')\n",
    "df_validation['order_date']=pd.to_datetime(df_validation['order_date'],format = '%m/%d/%Y')\n",
    "df_validation['user_reg_date']=pd.to_datetime(df_validation['user_reg_date'],format = '%m/%d/%Y')\n",
    "df_validation['user_dob']=pd.to_datetime(df_validation['user_dob'],format = '%m/%d/%Y')\n",
    "df_validation['Today'] =pd.to_datetime(date.today(),format = '%Y-%m-%d')\n",
    "\n",
    "\n",
    "df_validation['Delivery_Time'] = (df_validation['delivery_date'] -df_validation['order_date'])// timedelta(days=1)\n",
    "df_validation['User_Age_years'] = (df_validation['Today'] - df_validation['user_dob'])// timedelta(days=365.2425)\n",
    "df_validation['System_Age_months'] =(df_validation['Today'] - df_validation['user_reg_date'])// timedelta(days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation['Delivery_Time'] = np.where(df_validation['Delivery_Time']< 0, df_validation['Delivery_Time'].mean(),df_validation['Delivery_Time'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         3.000000\n",
       "1         9.000000\n",
       "2         3.000000\n",
       "3         6.269396\n",
       "4         6.269396\n",
       "           ...    \n",
       "20050     2.000000\n",
       "20051    66.000000\n",
       "20052     3.000000\n",
       "20053     1.000000\n",
       "20054     1.000000\n",
       "Name: Delivery_Time, Length: 20055, dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation['Delivery_Time'] = df_validation['Delivery_Time'].fillna(df_validation['Delivery_Time'].mean())\n",
    "df_validation['Delivery_Time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Addition of variables using the historical return rates to the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "##### Addtiion of User Return History\n",
    "\n",
    "test = df[['user_return_history','order_date','user_id']]\n",
    "test.drop_duplicates(keep = False, inplace = True)\n",
    "test2 =  df_validation[['order_date','user_id']]\n",
    "test2.drop_duplicates(keep = False, inplace = True)\n",
    "test = test.append(test2)\n",
    "test = test.sort_values(by=['user_id','order_date'])\n",
    "test.drop_duplicates(keep = False, inplace = True)\n",
    "test['user_return_history'] = test['user_return_history'].ffill()\n",
    "test =test.drop_duplicates(subset=['order_date', 'user_id'])\n",
    "df_validation = df_validation.merge(test, how ='left', on=['user_id','order_date'])\n",
    "df_validation['user_return_history']= df_validation['user_return_history'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_size</th>\n",
       "      <th>item_color</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_title</th>\n",
       "      <th>user_dob</th>\n",
       "      <th>user_state</th>\n",
       "      <th>user_reg_date</th>\n",
       "      <th>Today</th>\n",
       "      <th>Delivery_Time</th>\n",
       "      <th>User_Age_years</th>\n",
       "      <th>System_Age_months</th>\n",
       "      <th>user_return_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>92</td>\n",
       "      <td>xl</td>\n",
       "      <td>turquoise</td>\n",
       "      <td>42</td>\n",
       "      <td>69.90</td>\n",
       "      <td>9392</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1962-04-14</td>\n",
       "      <td>1010</td>\n",
       "      <td>2016-03-24</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>2</td>\n",
       "      <td>xxl</td>\n",
       "      <td>green</td>\n",
       "      <td>2</td>\n",
       "      <td>19.90</td>\n",
       "      <td>9392</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1962-04-14</td>\n",
       "      <td>1010</td>\n",
       "      <td>2016-03-24</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>895</td>\n",
       "      <td>38</td>\n",
       "      <td>white</td>\n",
       "      <td>39</td>\n",
       "      <td>39.95</td>\n",
       "      <td>30826</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1964-04-27</td>\n",
       "      <td>1001</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5</td>\n",
       "      <td>l</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "      <td>69.90</td>\n",
       "      <td>30828</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1966-05-13</td>\n",
       "      <td>1008</td>\n",
       "      <td>2016-01-21</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>6.269396</td>\n",
       "      <td>55.0</td>\n",
       "      <td>66</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>NaT</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>purple</td>\n",
       "      <td>1</td>\n",
       "      <td>89.90</td>\n",
       "      <td>22948</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1957-03-11</td>\n",
       "      <td>1001</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>6.269396</td>\n",
       "      <td>64.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20050</th>\n",
       "      <td>99935</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-13</td>\n",
       "      <td>2217</td>\n",
       "      <td>128</td>\n",
       "      <td>red</td>\n",
       "      <td>42</td>\n",
       "      <td>79.90</td>\n",
       "      <td>48229</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1965-11-15</td>\n",
       "      <td>1001</td>\n",
       "      <td>2015-10-06</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051</th>\n",
       "      <td>99943</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-11-16</td>\n",
       "      <td>1652</td>\n",
       "      <td>39</td>\n",
       "      <td>purple</td>\n",
       "      <td>47</td>\n",
       "      <td>39.90</td>\n",
       "      <td>48233</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1959-06-30</td>\n",
       "      <td>1008</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20052</th>\n",
       "      <td>99952</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-14</td>\n",
       "      <td>1611</td>\n",
       "      <td>40</td>\n",
       "      <td>black</td>\n",
       "      <td>70</td>\n",
       "      <td>249.90</td>\n",
       "      <td>48234</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1962-10-02</td>\n",
       "      <td>1007</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20053</th>\n",
       "      <td>99986</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>1550</td>\n",
       "      <td>l</td>\n",
       "      <td>berry</td>\n",
       "      <td>117</td>\n",
       "      <td>129.90</td>\n",
       "      <td>12130</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1008</td>\n",
       "      <td>2016-01-20</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20054</th>\n",
       "      <td>99996</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>156</td>\n",
       "      <td>20</td>\n",
       "      <td>blue</td>\n",
       "      <td>34</td>\n",
       "      <td>29.90</td>\n",
       "      <td>713</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1959-03-21</td>\n",
       "      <td>1011</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20055 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       order_item_id order_date delivery_date  item_id item_size item_color  \\\n",
       "0                 26 2016-06-23    2016-06-26       92        xl  turquoise   \n",
       "1                 28 2016-06-23    2016-07-02        2       xxl      green   \n",
       "2                 37 2016-06-23    2016-06-26      895        38      white   \n",
       "3                 56 2016-06-23           NaT        5         l      white   \n",
       "4                 65 2016-06-23           NaT       55        40     purple   \n",
       "...              ...        ...           ...      ...       ...        ...   \n",
       "20050          99935 2016-09-11    2016-09-13     2217       128        red   \n",
       "20051          99943 2016-09-11    2016-11-16     1652        39     purple   \n",
       "20052          99952 2016-09-11    2016-09-14     1611        40      black   \n",
       "20053          99986 2016-09-11    2016-09-12     1550         l      berry   \n",
       "20054          99996 2016-09-11    2016-09-12      156        20       blue   \n",
       "\n",
       "       brand_id  item_price  user_id user_title   user_dob  user_state  \\\n",
       "0            42       69.90     9392        Mrs 1962-04-14        1010   \n",
       "1             2       19.90     9392        Mrs 1962-04-14        1010   \n",
       "2            39       39.95    30826        Mrs 1964-04-27        1001   \n",
       "3             5       69.90    30828        Mrs 1966-05-13        1008   \n",
       "4             1       89.90    22948        Mrs 1957-03-11        1001   \n",
       "...         ...         ...      ...        ...        ...         ...   \n",
       "20050        42       79.90    48229        Mrs 1965-11-15        1001   \n",
       "20051        47       39.90    48233        Mrs 1959-06-30        1008   \n",
       "20052        70      249.90    48234        Mrs 1962-10-02        1007   \n",
       "20053       117      129.90    12130        Mrs        NaT        1008   \n",
       "20054        34       29.90      713        Mrs 1959-03-21        1011   \n",
       "\n",
       "      user_reg_date      Today  Delivery_Time  User_Age_years  \\\n",
       "0        2016-03-24 2021-07-06       3.000000            59.0   \n",
       "1        2016-03-24 2021-07-06       9.000000            59.0   \n",
       "2        2015-02-17 2021-07-06       3.000000            57.0   \n",
       "3        2016-01-21 2021-07-06       6.269396            55.0   \n",
       "4        2015-02-17 2021-07-06       6.269396            64.0   \n",
       "...             ...        ...            ...             ...   \n",
       "20050    2015-10-06 2021-07-06       2.000000            55.0   \n",
       "20051    2016-09-12 2021-07-06      66.000000            62.0   \n",
       "20052    2016-09-12 2021-07-06       3.000000            58.0   \n",
       "20053    2016-01-20 2021-07-06       1.000000             NaN   \n",
       "20054    2015-02-17 2021-07-06       1.000000            62.0   \n",
       "\n",
       "       System_Age_months  user_return_history  \n",
       "0                     64                  0.0  \n",
       "1                     64                  0.0  \n",
       "2                     77                  0.0  \n",
       "3                     66                  0.0  \n",
       "4                     77                  0.0  \n",
       "...                  ...                  ...  \n",
       "20050                 70                  0.0  \n",
       "20051                 58                  0.0  \n",
       "20052                 58                  0.0  \n",
       "20053                 66                  0.0  \n",
       "20054                 77                  0.0  \n",
       "\n",
       "[20055 rows x 18 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20055.000000\n",
       "mean         0.421840\n",
       "std          1.606764\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max         48.000000\n",
       "Name: user_return_history, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation['user_return_history'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adding the other derived trend variables using the traning data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Avinash Mishra\\Avinash\\anaconda20\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "variable_list = ['brand_id','item_id','item_size','user_state','item_color']\n",
    "\n",
    "for each in variable_list :\n",
    "    \n",
    "    var = str(each) + \"_return_trend\"\n",
    "    \n",
    "    test = df[['order_date',var,each]]\n",
    "    \n",
    "    \n",
    "    test.drop_duplicates(keep = False, inplace = True)\n",
    "    \n",
    "    test2 =  df_validation[['order_date',each]]\n",
    "    \n",
    "    test2.drop_duplicates(keep = False, inplace = True)\n",
    "    \n",
    "    \n",
    "    test = test.append(test2)\n",
    "    \n",
    "    test = test.sort_values(by=[each,'order_date'])\n",
    "    \n",
    "    \n",
    "    test.drop_duplicates(keep = False, inplace = True)\n",
    "    \n",
    "    test[var] = test[var].ffill()\n",
    "    \n",
    "    test =test.drop_duplicates(subset=['order_date', each])\n",
    "    \n",
    "    \n",
    "    df_validation = df_validation.merge(test, how ='left', on=[each,'order_date'])\n",
    "    df_validation[var]= df_validation[var].fillna(0)\n",
    "\n",
    "\n",
    "df_validation ['Price'] = df_validation ['item_price'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADDING THE DUMMY VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_updated =pd.get_dummies(df_validation, columns=[\"item_id\",\"item_size\",\"item_color\",\"brand_id\",\"user_title\",\"user_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>item_price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_dob</th>\n",
       "      <th>user_reg_date</th>\n",
       "      <th>Today</th>\n",
       "      <th>Delivery_Time</th>\n",
       "      <th>User_Age_years</th>\n",
       "      <th>...</th>\n",
       "      <th>user_state_1007</th>\n",
       "      <th>user_state_1008</th>\n",
       "      <th>user_state_1009</th>\n",
       "      <th>user_state_1010</th>\n",
       "      <th>user_state_1011</th>\n",
       "      <th>user_state_1012</th>\n",
       "      <th>user_state_1013</th>\n",
       "      <th>user_state_1014</th>\n",
       "      <th>user_state_1015</th>\n",
       "      <th>user_state_1016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>69.90</td>\n",
       "      <td>9392</td>\n",
       "      <td>1962-04-14</td>\n",
       "      <td>2016-03-24</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>19.90</td>\n",
       "      <td>9392</td>\n",
       "      <td>1962-04-14</td>\n",
       "      <td>2016-03-24</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>39.95</td>\n",
       "      <td>30826</td>\n",
       "      <td>1964-04-27</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>NaT</td>\n",
       "      <td>69.90</td>\n",
       "      <td>30828</td>\n",
       "      <td>1966-05-13</td>\n",
       "      <td>2016-01-21</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>6.269396</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>NaT</td>\n",
       "      <td>89.90</td>\n",
       "      <td>22948</td>\n",
       "      <td>1957-03-11</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>6.269396</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20050</th>\n",
       "      <td>99935</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-13</td>\n",
       "      <td>79.90</td>\n",
       "      <td>48229</td>\n",
       "      <td>1965-11-15</td>\n",
       "      <td>2015-10-06</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051</th>\n",
       "      <td>99943</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-11-16</td>\n",
       "      <td>39.90</td>\n",
       "      <td>48233</td>\n",
       "      <td>1959-06-30</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20052</th>\n",
       "      <td>99952</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-14</td>\n",
       "      <td>249.90</td>\n",
       "      <td>48234</td>\n",
       "      <td>1962-10-02</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20053</th>\n",
       "      <td>99986</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>129.90</td>\n",
       "      <td>12130</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2016-01-20</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20054</th>\n",
       "      <td>99996</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>29.90</td>\n",
       "      <td>713</td>\n",
       "      <td>1959-03-21</td>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20055 rows × 1954 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       order_item_id order_date delivery_date  item_price  user_id   user_dob  \\\n",
       "0                 26 2016-06-23    2016-06-26       69.90     9392 1962-04-14   \n",
       "1                 28 2016-06-23    2016-07-02       19.90     9392 1962-04-14   \n",
       "2                 37 2016-06-23    2016-06-26       39.95    30826 1964-04-27   \n",
       "3                 56 2016-06-23           NaT       69.90    30828 1966-05-13   \n",
       "4                 65 2016-06-23           NaT       89.90    22948 1957-03-11   \n",
       "...              ...        ...           ...         ...      ...        ...   \n",
       "20050          99935 2016-09-11    2016-09-13       79.90    48229 1965-11-15   \n",
       "20051          99943 2016-09-11    2016-11-16       39.90    48233 1959-06-30   \n",
       "20052          99952 2016-09-11    2016-09-14      249.90    48234 1962-10-02   \n",
       "20053          99986 2016-09-11    2016-09-12      129.90    12130        NaT   \n",
       "20054          99996 2016-09-11    2016-09-12       29.90      713 1959-03-21   \n",
       "\n",
       "      user_reg_date      Today  Delivery_Time  User_Age_years  ...  \\\n",
       "0        2016-03-24 2021-07-06       3.000000            59.0  ...   \n",
       "1        2016-03-24 2021-07-06       9.000000            59.0  ...   \n",
       "2        2015-02-17 2021-07-06       3.000000            57.0  ...   \n",
       "3        2016-01-21 2021-07-06       6.269396            55.0  ...   \n",
       "4        2015-02-17 2021-07-06       6.269396            64.0  ...   \n",
       "...             ...        ...            ...             ...  ...   \n",
       "20050    2015-10-06 2021-07-06       2.000000            55.0  ...   \n",
       "20051    2016-09-12 2021-07-06      66.000000            62.0  ...   \n",
       "20052    2016-09-12 2021-07-06       3.000000            58.0  ...   \n",
       "20053    2016-01-20 2021-07-06       1.000000             NaN  ...   \n",
       "20054    2015-02-17 2021-07-06       1.000000            62.0  ...   \n",
       "\n",
       "       user_state_1007  user_state_1008  user_state_1009  user_state_1010  \\\n",
       "0                    0                0                0                1   \n",
       "1                    0                0                0                1   \n",
       "2                    0                0                0                0   \n",
       "3                    0                1                0                0   \n",
       "4                    0                0                0                0   \n",
       "...                ...              ...              ...              ...   \n",
       "20050                0                0                0                0   \n",
       "20051                0                1                0                0   \n",
       "20052                1                0                0                0   \n",
       "20053                0                1                0                0   \n",
       "20054                0                0                0                0   \n",
       "\n",
       "       user_state_1011  user_state_1012  user_state_1013  user_state_1014  \\\n",
       "0                    0                0                0                0   \n",
       "1                    0                0                0                0   \n",
       "2                    0                0                0                0   \n",
       "3                    0                0                0                0   \n",
       "4                    0                0                0                0   \n",
       "...                ...              ...              ...              ...   \n",
       "20050                0                0                0                0   \n",
       "20051                0                0                0                0   \n",
       "20052                0                0                0                0   \n",
       "20053                0                0                0                0   \n",
       "20054                1                0                0                0   \n",
       "\n",
       "       user_state_1015  user_state_1016  \n",
       "0                    0                0  \n",
       "1                    0                0  \n",
       "2                    0                0  \n",
       "3                    0                0  \n",
       "4                    0                0  \n",
       "...                ...              ...  \n",
       "20050                0                0  \n",
       "20051                0                0  \n",
       "20052                0                0  \n",
       "20053                0                0  \n",
       "20054                0                0  \n",
       "\n",
       "[20055 rows x 1954 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "20050    0.0\n",
       "20051    0.0\n",
       "20052    0.0\n",
       "20053    0.0\n",
       "20054    0.0\n",
       "Name: user_return_history, Length: 20055, dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation['user_return_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_updated['type'] = 0\n",
    "df_updated['type'] = 1\n",
    "df_overall= df_updated.append(df_validation_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall = df_overall.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Delivery_Time', 'User_Age_years', 'System_Age_months',\n",
       "       'user_return_history', 'brand_id_return_trend', 'item_id_return_trend',\n",
       "       'item_size_return_trend', 'user_state_return_trend',\n",
       "       'item_color_return_trend', 'item_id_1',\n",
       "       ...\n",
       "       'item_id_1433', 'item_id_1449', 'item_id_1454', 'item_id_1829',\n",
       "       'item_id_2176', 'item_id_2189', 'item_id_2210', 'item_id_2222',\n",
       "       'item_id_2226', 'brand_id_109'],\n",
       "      dtype='object', length=2289)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_overall.columns[11:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the overall data for standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_validation =  pd.DataFrame(sc.fit_transform(df_overall.iloc[:,11:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation.columns =df_overall.iloc[:,11:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = X_validation[X_validation['type']<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = X_validation.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Delivery_Time', 'User_Age_years', 'System_Age_months',\n",
       "       'user_return_history', 'brand_id_return_trend', 'item_id_return_trend',\n",
       "       'item_size_return_trend', 'user_state_return_trend',\n",
       "       'item_color_return_trend',\n",
       "       ...\n",
       "       'item_id_1433', 'item_id_1449', 'item_id_1454', 'item_id_1829',\n",
       "       'item_id_2176', 'item_id_2189', 'item_id_2210', 'item_id_2222',\n",
       "       'item_id_2226', 'brand_id_109'],\n",
       "      dtype='object', length=2290)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Delivery_Time', 'User_Age_years', 'System_Age_months',\n",
       "       'user_return_history', 'brand_id_return_trend', 'item_id_return_trend',\n",
       "       'item_size_return_trend', 'user_state_return_trend',\n",
       "       'item_color_return_trend', 'item_id_1',\n",
       "       ...\n",
       "       'user_state_1008', 'user_state_1009', 'user_state_1010',\n",
       "       'user_state_1011', 'user_state_1012', 'user_state_1013',\n",
       "       'user_state_1014', 'user_state_1015', 'user_state_1016', 'Price'],\n",
       "      dtype='object', length=2246)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Delivery_Time</th>\n",
       "      <th>User_Age_years</th>\n",
       "      <th>System_Age_months</th>\n",
       "      <th>user_return_history</th>\n",
       "      <th>brand_id_return_trend</th>\n",
       "      <th>item_id_return_trend</th>\n",
       "      <th>item_size_return_trend</th>\n",
       "      <th>user_state_return_trend</th>\n",
       "      <th>item_color_return_trend</th>\n",
       "      <th>item_id_1</th>\n",
       "      <th>...</th>\n",
       "      <th>user_state_1008</th>\n",
       "      <th>user_state_1009</th>\n",
       "      <th>user_state_1010</th>\n",
       "      <th>user_state_1011</th>\n",
       "      <th>user_state_1012</th>\n",
       "      <th>user_state_1013</th>\n",
       "      <th>user_state_1014</th>\n",
       "      <th>user_state_1015</th>\n",
       "      <th>user_state_1016</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.196971</td>\n",
       "      <td>0.244953</td>\n",
       "      <td>-0.693001</td>\n",
       "      <td>-0.316282</td>\n",
       "      <td>-1.541751</td>\n",
       "      <td>-1.391217</td>\n",
       "      <td>-1.647941</td>\n",
       "      <td>-1.713599</td>\n",
       "      <td>-1.682406</td>\n",
       "      <td>-0.040902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.421944</td>\n",
       "      <td>-0.127728</td>\n",
       "      <td>1.836075</td>\n",
       "      <td>-0.225216</td>\n",
       "      <td>-0.095363</td>\n",
       "      <td>-0.18165</td>\n",
       "      <td>-0.104934</td>\n",
       "      <td>-0.24203</td>\n",
       "      <td>-0.127117</td>\n",
       "      <td>0.095002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.164089</td>\n",
       "      <td>0.244953</td>\n",
       "      <td>-0.693001</td>\n",
       "      <td>-0.316282</td>\n",
       "      <td>-1.541751</td>\n",
       "      <td>1.243709</td>\n",
       "      <td>-1.647941</td>\n",
       "      <td>-1.713599</td>\n",
       "      <td>-1.682406</td>\n",
       "      <td>-0.040902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.421944</td>\n",
       "      <td>-0.127728</td>\n",
       "      <td>1.836075</td>\n",
       "      <td>-0.225216</td>\n",
       "      <td>-0.095363</td>\n",
       "      <td>-0.18165</td>\n",
       "      <td>-0.104934</td>\n",
       "      <td>-0.24203</td>\n",
       "      <td>-0.127117</td>\n",
       "      <td>-0.942197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.196971</td>\n",
       "      <td>0.099556</td>\n",
       "      <td>1.079185</td>\n",
       "      <td>-0.316282</td>\n",
       "      <td>-1.541751</td>\n",
       "      <td>-1.391217</td>\n",
       "      <td>-1.647941</td>\n",
       "      <td>-1.713599</td>\n",
       "      <td>-1.682406</td>\n",
       "      <td>-0.040902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.421944</td>\n",
       "      <td>-0.127728</td>\n",
       "      <td>-0.544640</td>\n",
       "      <td>-0.225216</td>\n",
       "      <td>-0.095363</td>\n",
       "      <td>-0.18165</td>\n",
       "      <td>-0.104934</td>\n",
       "      <td>-0.24203</td>\n",
       "      <td>-0.127117</td>\n",
       "      <td>-0.526280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000230</td>\n",
       "      <td>-0.045841</td>\n",
       "      <td>-0.420357</td>\n",
       "      <td>-0.316282</td>\n",
       "      <td>-1.541751</td>\n",
       "      <td>-1.391217</td>\n",
       "      <td>-1.647941</td>\n",
       "      <td>-1.713599</td>\n",
       "      <td>-1.682406</td>\n",
       "      <td>-0.040902</td>\n",
       "      <td>...</td>\n",
       "      <td>2.369980</td>\n",
       "      <td>-0.127728</td>\n",
       "      <td>-0.544640</td>\n",
       "      <td>-0.225216</td>\n",
       "      <td>-0.095363</td>\n",
       "      <td>-0.18165</td>\n",
       "      <td>-0.104934</td>\n",
       "      <td>-0.24203</td>\n",
       "      <td>-0.127117</td>\n",
       "      <td>0.095002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.608446</td>\n",
       "      <td>1.079185</td>\n",
       "      <td>-0.316282</td>\n",
       "      <td>-1.541751</td>\n",
       "      <td>-1.391217</td>\n",
       "      <td>-1.647941</td>\n",
       "      <td>-1.713599</td>\n",
       "      <td>-1.682406</td>\n",
       "      <td>-0.040902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.421944</td>\n",
       "      <td>-0.127728</td>\n",
       "      <td>-0.544640</td>\n",
       "      <td>-0.225216</td>\n",
       "      <td>-0.095363</td>\n",
       "      <td>-0.18165</td>\n",
       "      <td>-0.104934</td>\n",
       "      <td>-0.24203</td>\n",
       "      <td>-0.127117</td>\n",
       "      <td>0.509882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20050</th>\n",
       "      <td>-0.257147</td>\n",
       "      <td>-0.045841</td>\n",
       "      <td>0.124931</td>\n",
       "      <td>-0.316282</td>\n",
       "      <td>-1.541751</td>\n",
       "      <td>-0.220139</td>\n",
       "      <td>-0.112835</td>\n",
       "      <td>-1.713599</td>\n",
       "      <td>-1.682406</td>\n",
       "      <td>-0.040902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.421944</td>\n",
       "      <td>-0.127728</td>\n",
       "      <td>-0.544640</td>\n",
       "      <td>-0.225216</td>\n",
       "      <td>-0.095363</td>\n",
       "      <td>-0.18165</td>\n",
       "      <td>-0.104934</td>\n",
       "      <td>-0.24203</td>\n",
       "      <td>-0.127117</td>\n",
       "      <td>0.302442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051</th>\n",
       "      <td>3.594154</td>\n",
       "      <td>0.463049</td>\n",
       "      <td>-1.510932</td>\n",
       "      <td>-0.316282</td>\n",
       "      <td>-1.541751</td>\n",
       "      <td>0.010528</td>\n",
       "      <td>-1.647941</td>\n",
       "      <td>-1.713599</td>\n",
       "      <td>-1.682406</td>\n",
       "      <td>-0.040902</td>\n",
       "      <td>...</td>\n",
       "      <td>2.369980</td>\n",
       "      <td>-0.127728</td>\n",
       "      <td>-0.544640</td>\n",
       "      <td>-0.225216</td>\n",
       "      <td>-0.095363</td>\n",
       "      <td>-0.18165</td>\n",
       "      <td>-0.104934</td>\n",
       "      <td>-0.24203</td>\n",
       "      <td>-0.127117</td>\n",
       "      <td>-0.527318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20052</th>\n",
       "      <td>-0.196971</td>\n",
       "      <td>0.172255</td>\n",
       "      <td>-1.510932</td>\n",
       "      <td>-0.316282</td>\n",
       "      <td>-1.541751</td>\n",
       "      <td>0.950939</td>\n",
       "      <td>-1.647941</td>\n",
       "      <td>-1.713599</td>\n",
       "      <td>-1.682406</td>\n",
       "      <td>-0.040902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.421944</td>\n",
       "      <td>-0.127728</td>\n",
       "      <td>-0.544640</td>\n",
       "      <td>-0.225216</td>\n",
       "      <td>-0.095363</td>\n",
       "      <td>-0.18165</td>\n",
       "      <td>-0.104934</td>\n",
       "      <td>-0.24203</td>\n",
       "      <td>-0.127117</td>\n",
       "      <td>3.828920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20053</th>\n",
       "      <td>-0.317324</td>\n",
       "      <td>-4.044258</td>\n",
       "      <td>-0.420357</td>\n",
       "      <td>-0.316282</td>\n",
       "      <td>-1.541751</td>\n",
       "      <td>-1.391217</td>\n",
       "      <td>-1.647941</td>\n",
       "      <td>-1.713599</td>\n",
       "      <td>-1.682406</td>\n",
       "      <td>-0.040902</td>\n",
       "      <td>...</td>\n",
       "      <td>2.369980</td>\n",
       "      <td>-0.127728</td>\n",
       "      <td>-0.544640</td>\n",
       "      <td>-0.225216</td>\n",
       "      <td>-0.095363</td>\n",
       "      <td>-0.18165</td>\n",
       "      <td>-0.104934</td>\n",
       "      <td>-0.24203</td>\n",
       "      <td>-0.127117</td>\n",
       "      <td>1.339642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20054</th>\n",
       "      <td>-0.317324</td>\n",
       "      <td>0.463049</td>\n",
       "      <td>1.079185</td>\n",
       "      <td>-0.316282</td>\n",
       "      <td>0.991253</td>\n",
       "      <td>0.950939</td>\n",
       "      <td>0.270942</td>\n",
       "      <td>-1.713599</td>\n",
       "      <td>-1.682406</td>\n",
       "      <td>-0.040902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.421944</td>\n",
       "      <td>-0.127728</td>\n",
       "      <td>-0.544640</td>\n",
       "      <td>4.440189</td>\n",
       "      <td>-0.095363</td>\n",
       "      <td>-0.18165</td>\n",
       "      <td>-0.104934</td>\n",
       "      <td>-0.24203</td>\n",
       "      <td>-0.127117</td>\n",
       "      <td>-0.734757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20055 rows × 2246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Delivery_Time  User_Age_years  System_Age_months  user_return_history  \\\n",
       "0          -0.196971        0.244953          -0.693001            -0.316282   \n",
       "1           0.164089        0.244953          -0.693001            -0.316282   \n",
       "2          -0.196971        0.099556           1.079185            -0.316282   \n",
       "3          -0.000230       -0.045841          -0.420357            -0.316282   \n",
       "4          -0.000230        0.608446           1.079185            -0.316282   \n",
       "...              ...             ...                ...                  ...   \n",
       "20050      -0.257147       -0.045841           0.124931            -0.316282   \n",
       "20051       3.594154        0.463049          -1.510932            -0.316282   \n",
       "20052      -0.196971        0.172255          -1.510932            -0.316282   \n",
       "20053      -0.317324       -4.044258          -0.420357            -0.316282   \n",
       "20054      -0.317324        0.463049           1.079185            -0.316282   \n",
       "\n",
       "       brand_id_return_trend  item_id_return_trend  item_size_return_trend  \\\n",
       "0                  -1.541751             -1.391217               -1.647941   \n",
       "1                  -1.541751              1.243709               -1.647941   \n",
       "2                  -1.541751             -1.391217               -1.647941   \n",
       "3                  -1.541751             -1.391217               -1.647941   \n",
       "4                  -1.541751             -1.391217               -1.647941   \n",
       "...                      ...                   ...                     ...   \n",
       "20050              -1.541751             -0.220139               -0.112835   \n",
       "20051              -1.541751              0.010528               -1.647941   \n",
       "20052              -1.541751              0.950939               -1.647941   \n",
       "20053              -1.541751             -1.391217               -1.647941   \n",
       "20054               0.991253              0.950939                0.270942   \n",
       "\n",
       "       user_state_return_trend  item_color_return_trend  item_id_1  ...  \\\n",
       "0                    -1.713599                -1.682406  -0.040902  ...   \n",
       "1                    -1.713599                -1.682406  -0.040902  ...   \n",
       "2                    -1.713599                -1.682406  -0.040902  ...   \n",
       "3                    -1.713599                -1.682406  -0.040902  ...   \n",
       "4                    -1.713599                -1.682406  -0.040902  ...   \n",
       "...                        ...                      ...        ...  ...   \n",
       "20050                -1.713599                -1.682406  -0.040902  ...   \n",
       "20051                -1.713599                -1.682406  -0.040902  ...   \n",
       "20052                -1.713599                -1.682406  -0.040902  ...   \n",
       "20053                -1.713599                -1.682406  -0.040902  ...   \n",
       "20054                -1.713599                -1.682406  -0.040902  ...   \n",
       "\n",
       "       user_state_1008  user_state_1009  user_state_1010  user_state_1011  \\\n",
       "0            -0.421944        -0.127728         1.836075        -0.225216   \n",
       "1            -0.421944        -0.127728         1.836075        -0.225216   \n",
       "2            -0.421944        -0.127728        -0.544640        -0.225216   \n",
       "3             2.369980        -0.127728        -0.544640        -0.225216   \n",
       "4            -0.421944        -0.127728        -0.544640        -0.225216   \n",
       "...                ...              ...              ...              ...   \n",
       "20050        -0.421944        -0.127728        -0.544640        -0.225216   \n",
       "20051         2.369980        -0.127728        -0.544640        -0.225216   \n",
       "20052        -0.421944        -0.127728        -0.544640        -0.225216   \n",
       "20053         2.369980        -0.127728        -0.544640        -0.225216   \n",
       "20054        -0.421944        -0.127728        -0.544640         4.440189   \n",
       "\n",
       "       user_state_1012  user_state_1013  user_state_1014  user_state_1015  \\\n",
       "0            -0.095363         -0.18165        -0.104934         -0.24203   \n",
       "1            -0.095363         -0.18165        -0.104934         -0.24203   \n",
       "2            -0.095363         -0.18165        -0.104934         -0.24203   \n",
       "3            -0.095363         -0.18165        -0.104934         -0.24203   \n",
       "4            -0.095363         -0.18165        -0.104934         -0.24203   \n",
       "...                ...              ...              ...              ...   \n",
       "20050        -0.095363         -0.18165        -0.104934         -0.24203   \n",
       "20051        -0.095363         -0.18165        -0.104934         -0.24203   \n",
       "20052        -0.095363         -0.18165        -0.104934         -0.24203   \n",
       "20053        -0.095363         -0.18165        -0.104934         -0.24203   \n",
       "20054        -0.095363         -0.18165        -0.104934         -0.24203   \n",
       "\n",
       "       user_state_1016     Price  \n",
       "0            -0.127117  0.095002  \n",
       "1            -0.127117 -0.942197  \n",
       "2            -0.127117 -0.526280  \n",
       "3            -0.127117  0.095002  \n",
       "4            -0.127117  0.509882  \n",
       "...                ...       ...  \n",
       "20050        -0.127117  0.302442  \n",
       "20051        -0.127117 -0.527318  \n",
       "20052        -0.127117  3.828920  \n",
       "20053        -0.127117  1.339642  \n",
       "20054        -0.127117 -0.734757  \n",
       "\n",
       "[20055 rows x 2246 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation=X_validation[X.columns]\n",
    "X_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction of the valdiation set using the Random Forrest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.546772</td>\n",
       "      <td>0.453228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.568091</td>\n",
       "      <td>0.431909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.498241</td>\n",
       "      <td>0.501759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.531982</td>\n",
       "      <td>0.468018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.535572</td>\n",
       "      <td>0.464428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20050</th>\n",
       "      <td>0.532646</td>\n",
       "      <td>0.467354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051</th>\n",
       "      <td>0.562396</td>\n",
       "      <td>0.437604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20052</th>\n",
       "      <td>0.519759</td>\n",
       "      <td>0.480241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20053</th>\n",
       "      <td>0.511933</td>\n",
       "      <td>0.488067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20054</th>\n",
       "      <td>0.485692</td>\n",
       "      <td>0.514308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20055 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "0      0.546772  0.453228\n",
       "1      0.568091  0.431909\n",
       "2      0.498241  0.501759\n",
       "3      0.531982  0.468018\n",
       "4      0.535572  0.464428\n",
       "...         ...       ...\n",
       "20050  0.532646  0.467354\n",
       "20051  0.562396  0.437604\n",
       "20052  0.519759  0.480241\n",
       "20053  0.511933  0.488067\n",
       "20054  0.485692  0.514308\n",
       "\n",
       "[20055 rows x 2 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_predict = pd.DataFrame(model.predict_proba(X_validation))\n",
    "rfc_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc_predict.columns =['Prob_0','Prob_1']\n",
    "\n",
    "final_pred_validation =pd.DataFrame()\n",
    "\n",
    "\n",
    "final_pred_validation['Predicted'] = np.where(rfc_predict['Prob_1']>=0.48,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5902"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred_validation['Predicted'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_size</th>\n",
       "      <th>item_color</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>Delivery_Time</th>\n",
       "      <th>User_Age_years</th>\n",
       "      <th>System_Age_months</th>\n",
       "      <th>user_return_history</th>\n",
       "      <th>brand_id_return_trend</th>\n",
       "      <th>item_id_return_trend</th>\n",
       "      <th>item_size_return_trend</th>\n",
       "      <th>user_state_return_trend</th>\n",
       "      <th>item_color_return_trend</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>92</td>\n",
       "      <td>xl</td>\n",
       "      <td>turquoise</td>\n",
       "      <td>42</td>\n",
       "      <td>69.90</td>\n",
       "      <td>9392</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>2</td>\n",
       "      <td>xxl</td>\n",
       "      <td>green</td>\n",
       "      <td>2</td>\n",
       "      <td>19.90</td>\n",
       "      <td>9392</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>895</td>\n",
       "      <td>38</td>\n",
       "      <td>white</td>\n",
       "      <td>39</td>\n",
       "      <td>39.95</td>\n",
       "      <td>30826</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>NaT</td>\n",
       "      <td>5</td>\n",
       "      <td>l</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "      <td>69.90</td>\n",
       "      <td>30828</td>\n",
       "      <td>...</td>\n",
       "      <td>6.269396</td>\n",
       "      <td>55.0</td>\n",
       "      <td>66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>2016-06-23</td>\n",
       "      <td>NaT</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>purple</td>\n",
       "      <td>1</td>\n",
       "      <td>89.90</td>\n",
       "      <td>22948</td>\n",
       "      <td>...</td>\n",
       "      <td>6.269396</td>\n",
       "      <td>64.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20050</th>\n",
       "      <td>0</td>\n",
       "      <td>99935</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-13</td>\n",
       "      <td>2217</td>\n",
       "      <td>128</td>\n",
       "      <td>red</td>\n",
       "      <td>42</td>\n",
       "      <td>79.90</td>\n",
       "      <td>48229</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051</th>\n",
       "      <td>0</td>\n",
       "      <td>99943</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-11-16</td>\n",
       "      <td>1652</td>\n",
       "      <td>39</td>\n",
       "      <td>purple</td>\n",
       "      <td>47</td>\n",
       "      <td>39.90</td>\n",
       "      <td>48233</td>\n",
       "      <td>...</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20052</th>\n",
       "      <td>1</td>\n",
       "      <td>99952</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-14</td>\n",
       "      <td>1611</td>\n",
       "      <td>40</td>\n",
       "      <td>black</td>\n",
       "      <td>70</td>\n",
       "      <td>249.90</td>\n",
       "      <td>48234</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20053</th>\n",
       "      <td>1</td>\n",
       "      <td>99986</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>1550</td>\n",
       "      <td>l</td>\n",
       "      <td>berry</td>\n",
       "      <td>117</td>\n",
       "      <td>129.90</td>\n",
       "      <td>12130</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20054</th>\n",
       "      <td>1</td>\n",
       "      <td>99996</td>\n",
       "      <td>2016-09-11</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>156</td>\n",
       "      <td>20</td>\n",
       "      <td>blue</td>\n",
       "      <td>34</td>\n",
       "      <td>29.90</td>\n",
       "      <td>713</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.594652</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20055 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Predicted  order_item_id order_date delivery_date  item_id item_size  \\\n",
       "0              0             26 2016-06-23    2016-06-26       92        xl   \n",
       "1              0             28 2016-06-23    2016-07-02        2       xxl   \n",
       "2              1             37 2016-06-23    2016-06-26      895        38   \n",
       "3              0             56 2016-06-23           NaT        5         l   \n",
       "4              0             65 2016-06-23           NaT       55        40   \n",
       "...          ...            ...        ...           ...      ...       ...   \n",
       "20050          0          99935 2016-09-11    2016-09-13     2217       128   \n",
       "20051          0          99943 2016-09-11    2016-11-16     1652        39   \n",
       "20052          1          99952 2016-09-11    2016-09-14     1611        40   \n",
       "20053          1          99986 2016-09-11    2016-09-12     1550         l   \n",
       "20054          1          99996 2016-09-11    2016-09-12      156        20   \n",
       "\n",
       "      item_color  brand_id  item_price  user_id  ... Delivery_Time  \\\n",
       "0      turquoise        42       69.90     9392  ...      3.000000   \n",
       "1          green         2       19.90     9392  ...      9.000000   \n",
       "2          white        39       39.95    30826  ...      3.000000   \n",
       "3          white         5       69.90    30828  ...      6.269396   \n",
       "4         purple         1       89.90    22948  ...      6.269396   \n",
       "...          ...       ...         ...      ...  ...           ...   \n",
       "20050        red        42       79.90    48229  ...      2.000000   \n",
       "20051     purple        47       39.90    48233  ...     66.000000   \n",
       "20052      black        70      249.90    48234  ...      3.000000   \n",
       "20053      berry       117      129.90    12130  ...      1.000000   \n",
       "20054       blue        34       29.90      713  ...      1.000000   \n",
       "\n",
       "      User_Age_years  System_Age_months user_return_history  \\\n",
       "0               59.0                 64                 0.0   \n",
       "1               59.0                 64                 0.0   \n",
       "2               57.0                 77                 0.0   \n",
       "3               55.0                 66                 0.0   \n",
       "4               64.0                 77                 0.0   \n",
       "...              ...                ...                 ...   \n",
       "20050           55.0                 70                 0.0   \n",
       "20051           62.0                 58                 0.0   \n",
       "20052           58.0                 58                 0.0   \n",
       "20053            NaN                 66                 0.0   \n",
       "20054           62.0                 77                 0.0   \n",
       "\n",
       "      brand_id_return_trend  item_id_return_trend  item_size_return_trend  \\\n",
       "0                  0.000000              0.000000                0.000000   \n",
       "1                  0.000000              0.750000                0.000000   \n",
       "2                  0.000000              0.000000                0.000000   \n",
       "3                  0.000000              0.000000                0.000000   \n",
       "4                  0.000000              0.000000                0.000000   \n",
       "...                     ...                   ...                     ...   \n",
       "20050              0.000000              0.333333                0.333333   \n",
       "20051              0.000000              0.398990                0.000000   \n",
       "20052              0.000000              0.666667                0.000000   \n",
       "20053              0.000000              0.000000                0.000000   \n",
       "20054              0.594652              0.666667                0.416667   \n",
       "\n",
       "       user_state_return_trend  item_color_return_trend   Price  \n",
       "0                          0.0                      0.0   69.90  \n",
       "1                          0.0                      0.0   19.90  \n",
       "2                          0.0                      0.0   39.95  \n",
       "3                          0.0                      0.0   69.90  \n",
       "4                          0.0                      0.0   89.90  \n",
       "...                        ...                      ...     ...  \n",
       "20050                      0.0                      0.0   79.90  \n",
       "20051                      0.0                      0.0   39.90  \n",
       "20052                      0.0                      0.0  249.90  \n",
       "20053                      0.0                      0.0  129.90  \n",
       "20054                      0.0                      0.0   29.90  \n",
       "\n",
       "[20055 rows x 25 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred_validation = pd.concat([final_pred_validation,df_validation],axis =1)\n",
    "final_pred_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_validation.to_csv(\"Final_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
